{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/malele/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (12038, 12, 10)\n",
      "Y_train shape: (12038, 52)\n",
      "X_test shape: (3009, 12, 10)\n",
      "Y_test shape: (3009, 52)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "import keras\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten, Conv2D, Conv1D, MaxPooling1D, concatenate, BatchNormalization\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)].T\n",
    "    return Y\n",
    "\n",
    "file = h5py.File('data/DB1_S1_image.h5','r')\n",
    "imageData   = file['imageData'][:]\n",
    "imageLabel  = file['imageLabel'][:]  \n",
    "file.close()\n",
    "\n",
    "# 随机打乱数据和标签\n",
    "N = imageData.shape[0]\n",
    "index = np.random.permutation(N)\n",
    "data  = imageData[index,:,:]\n",
    "label = imageLabel[index]\n",
    "\n",
    "# 对数据升维,标签one-hot\n",
    "# data  = np.expand_dims(data, axis=3)\n",
    "label = convert_to_one_hot(label,52).T\n",
    "\n",
    "# 划分数据集\n",
    "N = data.shape[0]\n",
    "num_train = round(N*0.8)\n",
    "X_train = data[0:num_train,:,:]\n",
    "Y_train = label[0:num_train,:]\n",
    "X_test  = data[num_train:N,:,:]\n",
    "Y_test  = label[num_train:N,:]\n",
    "\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#写一个LossHistory类，保存loss和acc\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = {'batch':[], 'epoch':[]}\n",
    "        self.accuracy = {'batch':[], 'epoch':[]}\n",
    "        self.val_loss = {'batch':[], 'epoch':[]}\n",
    "        self.val_acc = {'batch':[], 'epoch':[]}\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses['batch'].append(logs.get('loss'))\n",
    "        self.accuracy['batch'].append(logs.get('acc'))\n",
    "        self.val_loss['batch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['batch'].append(logs.get('val_acc'))\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses['epoch'].append(logs.get('loss'))\n",
    "        self.accuracy['epoch'].append(logs.get('acc'))\n",
    "        self.val_loss['epoch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['epoch'].append(logs.get('val_acc'))\n",
    "\n",
    "    def loss_plot(self, loss_type):\n",
    "        iters = range(len(self.losses[loss_type]))\n",
    "        plt.figure()\n",
    "        # acc\n",
    "        plt.plot(iters, self.accuracy[loss_type], 'r', label='train acc')\n",
    "        # loss\n",
    "        plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n",
    "        if loss_type == 'epoch':\n",
    "            # val_acc\n",
    "            plt.plot(iters, self.val_acc[loss_type], 'b', label='val acc')\n",
    "            # val_loss\n",
    "            plt.plot(iters, self.val_loss[loss_type], 'k', label='val loss')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(loss_type)\n",
    "        plt.ylabel('acc-loss')\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 12, 10)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 11, 128)      2688        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 10, 128)      3968        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 9, 128)       5248        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 8, 128)       6528        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1, 128)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 1, 128)       0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 1, 128)       0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1, 128)       0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 128)          0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 128)          0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 128)          0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 128)          0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512)          0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "fc1 (Dense)                     (None, 128)          65664       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           fc1[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "fc2 (Dense)                     (None, 52)           6708        dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 90,804\n",
      "Trainable params: 90,804\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def CNN(input_shape=(12,10), classes=52): \n",
    "    X_input = Input(input_shape)\n",
    "    \n",
    "    length=12\n",
    "    convs=[]\n",
    "    filter_sizes=[2, 3, 4, 5]\n",
    "    \n",
    "    for filters in filter_sizes:\n",
    "        layer_conv = Conv1D(filters=128, kernel_size=filters, activation='relu')(X_input)\n",
    "        layer_pool = MaxPooling1D(length-filters+1)(layer_conv)\n",
    "        layer_pool = Flatten()(layer_pool)\n",
    "        convs.append(layer_pool)\n",
    "    merge = concatenate(convs,axis=1)\n",
    "    \n",
    "    X = merge\n",
    "    #X = Dropout(0.5)(X)\n",
    "    X = Dense(128,activation='relu',name='fc1')(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = Dense(classes, activation='softmax', name='fc2')(X)\n",
    "    \n",
    "    model = Model(inputs=X_input, outputs=X, name='CNN')\n",
    "    return model\n",
    "    \n",
    "model = CNN(input_shape = (12, 10), classes = 52)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练原始数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12038 samples, validate on 3009 samples\n",
      "Epoch 1/200\n",
      "12038/12038 [==============================] - 2s 170us/step - loss: 3.3330 - acc: 0.1497 - val_loss: 2.7030 - val_acc: 0.3519\n",
      "Epoch 2/200\n",
      "12038/12038 [==============================] - 1s 98us/step - loss: 2.6599 - acc: 0.2952 - val_loss: 2.2652 - val_acc: 0.4327\n",
      "Epoch 3/200\n",
      "12038/12038 [==============================] - 1s 96us/step - loss: 2.3560 - acc: 0.3686 - val_loss: 2.0343 - val_acc: 0.4892\n",
      "Epoch 4/200\n",
      "12038/12038 [==============================] - 1s 107us/step - loss: 2.1570 - acc: 0.4121 - val_loss: 1.8395 - val_acc: 0.5321\n",
      "Epoch 5/200\n",
      "12038/12038 [==============================] - 1s 103us/step - loss: 1.9969 - acc: 0.4458 - val_loss: 1.7153 - val_acc: 0.5494\n",
      "Epoch 6/200\n",
      "12038/12038 [==============================] - 1s 100us/step - loss: 1.8735 - acc: 0.4774 - val_loss: 1.5893 - val_acc: 0.5723\n",
      "Epoch 7/200\n",
      "12038/12038 [==============================] - 1s 102us/step - loss: 1.7934 - acc: 0.4979 - val_loss: 1.5241 - val_acc: 0.5879\n",
      "Epoch 8/200\n",
      "12038/12038 [==============================] - 1s 100us/step - loss: 1.7089 - acc: 0.5135 - val_loss: 1.4556 - val_acc: 0.5979\n",
      "Epoch 9/200\n",
      "12038/12038 [==============================] - 1s 101us/step - loss: 1.6467 - acc: 0.5274 - val_loss: 1.3961 - val_acc: 0.6088\n",
      "Epoch 10/200\n",
      "12038/12038 [==============================] - 1s 110us/step - loss: 1.5983 - acc: 0.5397 - val_loss: 1.3412 - val_acc: 0.6175\n",
      "Epoch 11/200\n",
      "12038/12038 [==============================] - 1s 103us/step - loss: 1.5456 - acc: 0.5511 - val_loss: 1.3006 - val_acc: 0.6238\n",
      "Epoch 12/200\n",
      "12038/12038 [==============================] - 1s 102us/step - loss: 1.5077 - acc: 0.5628 - val_loss: 1.2705 - val_acc: 0.6321\n",
      "Epoch 13/200\n",
      "12038/12038 [==============================] - 1s 99us/step - loss: 1.4589 - acc: 0.5747 - val_loss: 1.2414 - val_acc: 0.6394\n",
      "Epoch 14/200\n",
      "12038/12038 [==============================] - 1s 100us/step - loss: 1.4245 - acc: 0.5837 - val_loss: 1.2009 - val_acc: 0.6474\n",
      "Epoch 15/200\n",
      "12038/12038 [==============================] - 1s 100us/step - loss: 1.4071 - acc: 0.5851 - val_loss: 1.1909 - val_acc: 0.6477\n",
      "Epoch 16/200\n",
      "12038/12038 [==============================] - 1s 107us/step - loss: 1.3786 - acc: 0.5909 - val_loss: 1.1652 - val_acc: 0.6437\n",
      "Epoch 17/200\n",
      "12038/12038 [==============================] - 1s 100us/step - loss: 1.3455 - acc: 0.5988 - val_loss: 1.1232 - val_acc: 0.6604\n",
      "Epoch 18/200\n",
      "12038/12038 [==============================] - 1s 107us/step - loss: 1.3351 - acc: 0.6010 - val_loss: 1.1262 - val_acc: 0.6617\n",
      "Epoch 19/200\n",
      "12038/12038 [==============================] - 1s 105us/step - loss: 1.3174 - acc: 0.6031 - val_loss: 1.1101 - val_acc: 0.6683\n",
      "Epoch 20/200\n",
      "12038/12038 [==============================] - 1s 106us/step - loss: 1.2857 - acc: 0.6126 - val_loss: 1.0776 - val_acc: 0.6760\n",
      "Epoch 21/200\n",
      "12038/12038 [==============================] - 1s 105us/step - loss: 1.2690 - acc: 0.6174 - val_loss: 1.0632 - val_acc: 0.6806\n",
      "Epoch 22/200\n",
      "12038/12038 [==============================] - 1s 102us/step - loss: 1.2584 - acc: 0.6210 - val_loss: 1.0541 - val_acc: 0.6810\n",
      "Epoch 23/200\n",
      "12038/12038 [==============================] - 1s 106us/step - loss: 1.2386 - acc: 0.6211 - val_loss: 1.0516 - val_acc: 0.6849\n",
      "Epoch 24/200\n",
      "12038/12038 [==============================] - 1s 104us/step - loss: 1.2308 - acc: 0.6229 - val_loss: 1.0209 - val_acc: 0.6949\n",
      "Epoch 25/200\n",
      "12038/12038 [==============================] - 1s 101us/step - loss: 1.2029 - acc: 0.6307 - val_loss: 1.0201 - val_acc: 0.6893\n",
      "Epoch 26/200\n",
      "12038/12038 [==============================] - 1s 103us/step - loss: 1.2018 - acc: 0.6352 - val_loss: 1.0059 - val_acc: 0.6956\n",
      "Epoch 27/200\n",
      "12038/12038 [==============================] - 1s 101us/step - loss: 1.1794 - acc: 0.6367 - val_loss: 0.9973 - val_acc: 0.6972\n",
      "Epoch 28/200\n",
      "12038/12038 [==============================] - 1s 101us/step - loss: 1.1655 - acc: 0.6446 - val_loss: 0.9884 - val_acc: 0.7032\n",
      "Epoch 29/200\n",
      "12038/12038 [==============================] - 1s 102us/step - loss: 1.1547 - acc: 0.6497 - val_loss: 0.9910 - val_acc: 0.6906\n",
      "Epoch 30/200\n",
      "12038/12038 [==============================] - 1s 104us/step - loss: 1.1404 - acc: 0.6470 - val_loss: 1.0055 - val_acc: 0.6899\n",
      "Epoch 31/200\n",
      "12038/12038 [==============================] - 1s 102us/step - loss: 1.1397 - acc: 0.6484 - val_loss: 0.9721 - val_acc: 0.7036\n",
      "Epoch 32/200\n",
      "12038/12038 [==============================] - 1s 103us/step - loss: 1.1286 - acc: 0.6520 - val_loss: 0.9498 - val_acc: 0.7168\n",
      "Epoch 33/200\n",
      "12038/12038 [==============================] - 1s 103us/step - loss: 1.1077 - acc: 0.6548 - val_loss: 0.9545 - val_acc: 0.7069\n",
      "Epoch 34/200\n",
      "12038/12038 [==============================] - 1s 104us/step - loss: 1.1004 - acc: 0.6528 - val_loss: 0.9570 - val_acc: 0.7102\n",
      "Epoch 35/200\n",
      "12038/12038 [==============================] - 1s 104us/step - loss: 1.0945 - acc: 0.6582 - val_loss: 0.9314 - val_acc: 0.7109\n",
      "Epoch 36/200\n",
      "12038/12038 [==============================] - 1s 108us/step - loss: 1.0847 - acc: 0.6612 - val_loss: 0.9456 - val_acc: 0.7132\n",
      "Epoch 37/200\n",
      "12038/12038 [==============================] - 1s 102us/step - loss: 1.0742 - acc: 0.6650 - val_loss: 0.9297 - val_acc: 0.7182\n",
      "Epoch 38/200\n",
      "12038/12038 [==============================] - 1s 101us/step - loss: 1.0544 - acc: 0.6717 - val_loss: 0.9341 - val_acc: 0.7082\n",
      "Epoch 39/200\n",
      "12038/12038 [==============================] - 1s 102us/step - loss: 1.0547 - acc: 0.6701 - val_loss: 0.9223 - val_acc: 0.7129\n",
      "Epoch 40/200\n",
      "12038/12038 [==============================] - 1s 105us/step - loss: 1.0466 - acc: 0.6716 - val_loss: 0.9139 - val_acc: 0.7222\n",
      "Epoch 41/200\n",
      "12038/12038 [==============================] - 1s 106us/step - loss: 1.0370 - acc: 0.6742 - val_loss: 0.9035 - val_acc: 0.7245\n",
      "Epoch 42/200\n",
      "12038/12038 [==============================] - 1s 103us/step - loss: 1.0282 - acc: 0.6786 - val_loss: 0.9111 - val_acc: 0.7232\n",
      "Epoch 43/200\n",
      "12038/12038 [==============================] - 1s 104us/step - loss: 1.0281 - acc: 0.6765 - val_loss: 0.8902 - val_acc: 0.7325\n",
      "Epoch 44/200\n",
      "12038/12038 [==============================] - 1s 103us/step - loss: 1.0188 - acc: 0.6743 - val_loss: 0.8856 - val_acc: 0.7235\n",
      "Epoch 45/200\n",
      "12038/12038 [==============================] - 1s 97us/step - loss: 1.0121 - acc: 0.6788 - val_loss: 0.8764 - val_acc: 0.7315\n",
      "Epoch 46/200\n",
      "12038/12038 [==============================] - 1s 102us/step - loss: 1.0055 - acc: 0.6831 - val_loss: 0.8798 - val_acc: 0.7285\n",
      "Epoch 47/200\n",
      "12038/12038 [==============================] - 1s 101us/step - loss: 0.9905 - acc: 0.6850 - val_loss: 0.8876 - val_acc: 0.7278\n",
      "Epoch 48/200\n",
      "12038/12038 [==============================] - 1s 102us/step - loss: 0.9911 - acc: 0.6815 - val_loss: 0.8734 - val_acc: 0.7308\n",
      "Epoch 49/200\n",
      "12038/12038 [==============================] - 1s 103us/step - loss: 0.9708 - acc: 0.6937 - val_loss: 0.8747 - val_acc: 0.7238\n",
      "Epoch 50/200\n",
      "12038/12038 [==============================] - 1s 102us/step - loss: 0.9745 - acc: 0.6917 - val_loss: 0.8723 - val_acc: 0.7328\n",
      "Epoch 51/200\n",
      "12038/12038 [==============================] - 1s 103us/step - loss: 0.9789 - acc: 0.6864 - val_loss: 0.8694 - val_acc: 0.7411\n",
      "Epoch 52/200\n",
      "12038/12038 [==============================] - 1s 107us/step - loss: 0.9582 - acc: 0.6972 - val_loss: 0.8612 - val_acc: 0.7421\n",
      "Epoch 53/200\n",
      "12038/12038 [==============================] - 1s 104us/step - loss: 0.9470 - acc: 0.7020 - val_loss: 0.8586 - val_acc: 0.7371\n",
      "Epoch 54/200\n",
      "12038/12038 [==============================] - 1s 97us/step - loss: 0.9455 - acc: 0.6965 - val_loss: 0.8544 - val_acc: 0.7368\n",
      "Epoch 55/200\n",
      "12038/12038 [==============================] - 1s 106us/step - loss: 0.9313 - acc: 0.7027 - val_loss: 0.8516 - val_acc: 0.7404\n",
      "Epoch 56/200\n",
      "12038/12038 [==============================] - 1s 104us/step - loss: 0.9256 - acc: 0.7021 - val_loss: 0.8537 - val_acc: 0.7398\n",
      "Epoch 57/200\n",
      "12038/12038 [==============================] - 1s 103us/step - loss: 0.9315 - acc: 0.7018 - val_loss: 0.8434 - val_acc: 0.7401\n",
      "Epoch 58/200\n",
      "12038/12038 [==============================] - 1s 108us/step - loss: 0.9210 - acc: 0.7016 - val_loss: 0.8577 - val_acc: 0.7385\n",
      "Epoch 59/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12038/12038 [==============================] - 1s 102us/step - loss: 0.9099 - acc: 0.7065 - val_loss: 0.8498 - val_acc: 0.7424\n",
      "Epoch 60/200\n",
      "12038/12038 [==============================] - 1s 101us/step - loss: 0.9085 - acc: 0.7098 - val_loss: 0.8401 - val_acc: 0.7404\n",
      "Epoch 61/200\n",
      "12038/12038 [==============================] - 1s 100us/step - loss: 0.9141 - acc: 0.7095 - val_loss: 0.8408 - val_acc: 0.7408\n",
      "Epoch 62/200\n",
      "12038/12038 [==============================] - 1s 105us/step - loss: 0.8999 - acc: 0.7095 - val_loss: 0.8380 - val_acc: 0.7418\n",
      "Epoch 63/200\n",
      "12038/12038 [==============================] - 1s 101us/step - loss: 0.9105 - acc: 0.7086 - val_loss: 0.8440 - val_acc: 0.7391\n",
      "Epoch 64/200\n",
      "12038/12038 [==============================] - 1s 106us/step - loss: 0.8920 - acc: 0.7096 - val_loss: 0.8281 - val_acc: 0.7414\n",
      "Epoch 65/200\n",
      "12038/12038 [==============================] - 1s 106us/step - loss: 0.8826 - acc: 0.7173 - val_loss: 0.8430 - val_acc: 0.7414\n",
      "Epoch 66/200\n",
      "12038/12038 [==============================] - 1s 103us/step - loss: 0.8762 - acc: 0.7168 - val_loss: 0.8393 - val_acc: 0.7438\n",
      "Epoch 67/200\n",
      "12038/12038 [==============================] - 1s 100us/step - loss: 0.8819 - acc: 0.7144 - val_loss: 0.8324 - val_acc: 0.7441\n",
      "Epoch 68/200\n",
      "12038/12038 [==============================] - 1s 104us/step - loss: 0.8853 - acc: 0.7145 - val_loss: 0.8332 - val_acc: 0.7478\n",
      "Epoch 69/200\n",
      "12038/12038 [==============================] - 1s 100us/step - loss: 0.8606 - acc: 0.7186 - val_loss: 0.8335 - val_acc: 0.7491\n",
      "Epoch 70/200\n",
      "12038/12038 [==============================] - 1s 103us/step - loss: 0.8664 - acc: 0.7231 - val_loss: 0.8265 - val_acc: 0.7491\n",
      "Epoch 71/200\n",
      "12038/12038 [==============================] - 1s 104us/step - loss: 0.8559 - acc: 0.7256 - val_loss: 0.8166 - val_acc: 0.7481\n",
      "Epoch 72/200\n",
      "12038/12038 [==============================] - 1s 100us/step - loss: 0.8627 - acc: 0.7197 - val_loss: 0.8191 - val_acc: 0.7494\n",
      "Epoch 73/200\n",
      "12038/12038 [==============================] - 1s 100us/step - loss: 0.8429 - acc: 0.7262 - val_loss: 0.8247 - val_acc: 0.7511\n",
      "Epoch 74/200\n",
      "12038/12038 [==============================] - 1s 101us/step - loss: 0.8498 - acc: 0.7255 - val_loss: 0.8113 - val_acc: 0.7501\n",
      "Epoch 75/200\n",
      "12038/12038 [==============================] - 1s 101us/step - loss: 0.8368 - acc: 0.7212 - val_loss: 0.8190 - val_acc: 0.7458\n",
      "Epoch 76/200\n",
      "12038/12038 [==============================] - 1s 101us/step - loss: 0.8358 - acc: 0.7284 - val_loss: 0.8157 - val_acc: 0.7501\n",
      "Epoch 77/200\n",
      "12038/12038 [==============================] - 1s 105us/step - loss: 0.8286 - acc: 0.7315 - val_loss: 0.8152 - val_acc: 0.7504\n",
      "Epoch 78/200\n",
      "12038/12038 [==============================] - 1s 103us/step - loss: 0.8409 - acc: 0.7259 - val_loss: 0.7979 - val_acc: 0.7554\n",
      "Epoch 79/200\n",
      "12038/12038 [==============================] - 1s 105us/step - loss: 0.8294 - acc: 0.7232 - val_loss: 0.8115 - val_acc: 0.7541\n",
      "Epoch 80/200\n",
      "12038/12038 [==============================] - 1s 101us/step - loss: 0.8147 - acc: 0.7397 - val_loss: 0.8020 - val_acc: 0.7524\n",
      "Epoch 81/200\n",
      "12038/12038 [==============================] - 1s 103us/step - loss: 0.8184 - acc: 0.7359 - val_loss: 0.7985 - val_acc: 0.7634\n",
      "Epoch 82/200\n",
      "12038/12038 [==============================] - 1s 108us/step - loss: 0.8173 - acc: 0.7345 - val_loss: 0.8139 - val_acc: 0.7554\n",
      "Epoch 83/200\n",
      "12038/12038 [==============================] - 1s 102us/step - loss: 0.8033 - acc: 0.7381 - val_loss: 0.8105 - val_acc: 0.7561\n",
      "Epoch 84/200\n",
      "12038/12038 [==============================] - 1s 104us/step - loss: 0.8063 - acc: 0.7378 - val_loss: 0.8134 - val_acc: 0.7504\n",
      "Epoch 85/200\n",
      "12038/12038 [==============================] - 1s 106us/step - loss: 0.8004 - acc: 0.7370 - val_loss: 0.8022 - val_acc: 0.7567\n",
      "Epoch 86/200\n",
      "12038/12038 [==============================] - 1s 107us/step - loss: 0.7890 - acc: 0.7414 - val_loss: 0.8075 - val_acc: 0.7574\n",
      "Epoch 87/200\n",
      "12038/12038 [==============================] - 1s 104us/step - loss: 0.7966 - acc: 0.7353 - val_loss: 0.8157 - val_acc: 0.7554\n",
      "Epoch 88/200\n",
      "12038/12038 [==============================] - 1s 102us/step - loss: 0.7825 - acc: 0.7431 - val_loss: 0.8062 - val_acc: 0.7557\n",
      "Epoch 89/200\n",
      "12038/12038 [==============================] - 1s 98us/step - loss: 0.7910 - acc: 0.7363 - val_loss: 0.8006 - val_acc: 0.7617\n",
      "Epoch 90/200\n",
      "12038/12038 [==============================] - 1s 104us/step - loss: 0.7855 - acc: 0.7420 - val_loss: 0.7994 - val_acc: 0.7660\n",
      "Epoch 91/200\n",
      "12038/12038 [==============================] - 1s 103us/step - loss: 0.7582 - acc: 0.7490 - val_loss: 0.8105 - val_acc: 0.7594\n",
      "Epoch 92/200\n",
      "12038/12038 [==============================] - 1s 101us/step - loss: 0.7738 - acc: 0.7459 - val_loss: 0.8018 - val_acc: 0.7591\n",
      "Epoch 93/200\n",
      "12038/12038 [==============================] - 1s 107us/step - loss: 0.7808 - acc: 0.7443 - val_loss: 0.8178 - val_acc: 0.7534\n",
      "Epoch 94/200\n",
      "12038/12038 [==============================] - 1s 101us/step - loss: 0.7861 - acc: 0.7446 - val_loss: 0.8002 - val_acc: 0.7567\n",
      "Epoch 95/200\n",
      "12038/12038 [==============================] - 1s 99us/step - loss: 0.7759 - acc: 0.7492 - val_loss: 0.7939 - val_acc: 0.7647\n",
      "Epoch 96/200\n",
      "12038/12038 [==============================] - 1s 105us/step - loss: 0.7613 - acc: 0.7457 - val_loss: 0.7957 - val_acc: 0.7547\n",
      "Epoch 97/200\n",
      "12038/12038 [==============================] - 1s 100us/step - loss: 0.7671 - acc: 0.7452 - val_loss: 0.7820 - val_acc: 0.7667\n",
      "Epoch 98/200\n",
      "12038/12038 [==============================] - 1s 103us/step - loss: 0.7475 - acc: 0.7513 - val_loss: 0.7965 - val_acc: 0.7620\n",
      "Epoch 99/200\n",
      "12038/12038 [==============================] - 1s 110us/step - loss: 0.7517 - acc: 0.7476 - val_loss: 0.8033 - val_acc: 0.7601\n",
      "Epoch 100/200\n",
      "12038/12038 [==============================] - 1s 101us/step - loss: 0.7375 - acc: 0.7591 - val_loss: 0.7918 - val_acc: 0.7617\n",
      "Epoch 101/200\n",
      "12038/12038 [==============================] - 1s 103us/step - loss: 0.7495 - acc: 0.7511 - val_loss: 0.7889 - val_acc: 0.7650\n",
      "Epoch 102/200\n",
      "12038/12038 [==============================] - 1s 102us/step - loss: 0.7404 - acc: 0.7555 - val_loss: 0.8044 - val_acc: 0.7604\n",
      "Epoch 103/200\n",
      "12038/12038 [==============================] - 1s 104us/step - loss: 0.7412 - acc: 0.7570 - val_loss: 0.7904 - val_acc: 0.7714\n",
      "Epoch 104/200\n",
      "12038/12038 [==============================] - 1s 100us/step - loss: 0.7346 - acc: 0.7556 - val_loss: 0.8062 - val_acc: 0.7617\n",
      "Epoch 105/200\n",
      "12038/12038 [==============================] - 1s 97us/step - loss: 0.7375 - acc: 0.7572 - val_loss: 0.7943 - val_acc: 0.7597\n",
      "Epoch 106/200\n",
      "12038/12038 [==============================] - 1s 105us/step - loss: 0.7290 - acc: 0.7577 - val_loss: 0.7872 - val_acc: 0.7640\n",
      "Epoch 107/200\n",
      "12038/12038 [==============================] - 1s 101us/step - loss: 0.7321 - acc: 0.7583 - val_loss: 0.7890 - val_acc: 0.7644\n",
      "Epoch 108/200\n",
      "12038/12038 [==============================] - 1s 100us/step - loss: 0.7208 - acc: 0.7662 - val_loss: 0.7917 - val_acc: 0.7650\n",
      "Epoch 109/200\n",
      "12038/12038 [==============================] - 1s 102us/step - loss: 0.7185 - acc: 0.7603 - val_loss: 0.7857 - val_acc: 0.7680\n",
      "Epoch 110/200\n",
      "12038/12038 [==============================] - 1s 104us/step - loss: 0.7295 - acc: 0.7583 - val_loss: 0.8000 - val_acc: 0.7630\n",
      "Epoch 111/200\n",
      "12038/12038 [==============================] - 1s 105us/step - loss: 0.7097 - acc: 0.7647 - val_loss: 0.8056 - val_acc: 0.7611\n",
      "Epoch 112/200\n",
      "12038/12038 [==============================] - 1s 107us/step - loss: 0.7125 - acc: 0.7613 - val_loss: 0.7920 - val_acc: 0.7690\n",
      "Epoch 113/200\n",
      "12038/12038 [==============================] - 1s 105us/step - loss: 0.6985 - acc: 0.7711 - val_loss: 0.7878 - val_acc: 0.7654\n",
      "Epoch 114/200\n",
      "12038/12038 [==============================] - 1s 105us/step - loss: 0.7133 - acc: 0.7623 - val_loss: 0.7948 - val_acc: 0.7584\n",
      "Epoch 115/200\n",
      "12038/12038 [==============================] - 1s 106us/step - loss: 0.7072 - acc: 0.7636 - val_loss: 0.7880 - val_acc: 0.7670\n",
      "Epoch 116/200\n",
      "12038/12038 [==============================] - 1s 103us/step - loss: 0.7031 - acc: 0.7635 - val_loss: 0.8045 - val_acc: 0.7617\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12038/12038 [==============================] - 1s 102us/step - loss: 0.7004 - acc: 0.7658 - val_loss: 0.7814 - val_acc: 0.7680\n",
      "Epoch 118/200\n",
      "12038/12038 [==============================] - 1s 104us/step - loss: 0.7131 - acc: 0.7623 - val_loss: 0.8088 - val_acc: 0.7660\n",
      "Epoch 119/200\n",
      "12038/12038 [==============================] - 1s 99us/step - loss: 0.7038 - acc: 0.7696 - val_loss: 0.7941 - val_acc: 0.7587\n",
      "Epoch 120/200\n",
      "12038/12038 [==============================] - 1s 104us/step - loss: 0.6896 - acc: 0.7661 - val_loss: 0.7901 - val_acc: 0.7664\n",
      "Epoch 121/200\n",
      "12038/12038 [==============================] - 1s 101us/step - loss: 0.6868 - acc: 0.7711 - val_loss: 0.7906 - val_acc: 0.7670\n",
      "Epoch 122/200\n",
      "12038/12038 [==============================] - 1s 104us/step - loss: 0.6827 - acc: 0.7723 - val_loss: 0.7897 - val_acc: 0.7680\n",
      "Epoch 123/200\n",
      "12038/12038 [==============================] - 1s 106us/step - loss: 0.6833 - acc: 0.7730 - val_loss: 0.7803 - val_acc: 0.7714\n",
      "Epoch 124/200\n",
      "12038/12038 [==============================] - 1s 105us/step - loss: 0.6711 - acc: 0.7771 - val_loss: 0.7929 - val_acc: 0.7684\n",
      "Epoch 125/200\n",
      "12038/12038 [==============================] - 1s 100us/step - loss: 0.6741 - acc: 0.7765 - val_loss: 0.7862 - val_acc: 0.7720\n",
      "Epoch 126/200\n",
      "12038/12038 [==============================] - 1s 103us/step - loss: 0.6832 - acc: 0.7736 - val_loss: 0.8024 - val_acc: 0.7574\n",
      "Epoch 127/200\n",
      "12038/12038 [==============================] - 1s 101us/step - loss: 0.6829 - acc: 0.7672 - val_loss: 0.7958 - val_acc: 0.7654\n",
      "Epoch 128/200\n",
      "12038/12038 [==============================] - 1s 104us/step - loss: 0.6640 - acc: 0.7791 - val_loss: 0.8052 - val_acc: 0.7630\n",
      "Epoch 129/200\n",
      "12038/12038 [==============================] - 1s 106us/step - loss: 0.6858 - acc: 0.7723 - val_loss: 0.7953 - val_acc: 0.7697\n",
      "Epoch 130/200\n",
      "12038/12038 [==============================] - 1s 101us/step - loss: 0.6683 - acc: 0.7794 - val_loss: 0.7777 - val_acc: 0.7690\n",
      "Epoch 131/200\n",
      "12038/12038 [==============================] - 1s 103us/step - loss: 0.6591 - acc: 0.7786 - val_loss: 0.7755 - val_acc: 0.7687\n",
      "Epoch 132/200\n",
      "12038/12038 [==============================] - 1s 105us/step - loss: 0.6675 - acc: 0.7781 - val_loss: 0.7689 - val_acc: 0.7710\n",
      "Epoch 133/200\n",
      "12038/12038 [==============================] - 1s 104us/step - loss: 0.6536 - acc: 0.7799 - val_loss: 0.7936 - val_acc: 0.7677\n",
      "Epoch 134/200\n",
      "12038/12038 [==============================] - 1s 101us/step - loss: 0.6675 - acc: 0.7748 - val_loss: 0.7980 - val_acc: 0.7664\n",
      "Epoch 135/200\n",
      "12038/12038 [==============================] - 1s 105us/step - loss: 0.6574 - acc: 0.7795 - val_loss: 0.7930 - val_acc: 0.7637\n",
      "Epoch 136/200\n",
      "12038/12038 [==============================] - 1s 102us/step - loss: 0.6527 - acc: 0.7794 - val_loss: 0.7941 - val_acc: 0.7654\n",
      "Epoch 137/200\n",
      "12038/12038 [==============================] - 1s 100us/step - loss: 0.6329 - acc: 0.7867 - val_loss: 0.8029 - val_acc: 0.7677\n",
      "Epoch 138/200\n",
      "12038/12038 [==============================] - 1s 106us/step - loss: 0.6483 - acc: 0.7798 - val_loss: 0.7860 - val_acc: 0.7730\n",
      "Epoch 139/200\n",
      "12038/12038 [==============================] - 1s 104us/step - loss: 0.6393 - acc: 0.7845 - val_loss: 0.8106 - val_acc: 0.7660\n",
      "Epoch 140/200\n",
      "12038/12038 [==============================] - 1s 102us/step - loss: 0.6346 - acc: 0.7863 - val_loss: 0.8005 - val_acc: 0.7697\n",
      "Epoch 141/200\n",
      "12038/12038 [==============================] - 1s 98us/step - loss: 0.6339 - acc: 0.7858 - val_loss: 0.7967 - val_acc: 0.7667\n",
      "Epoch 142/200\n",
      "12038/12038 [==============================] - 1s 106us/step - loss: 0.6365 - acc: 0.7850 - val_loss: 0.7939 - val_acc: 0.7700\n",
      "Epoch 143/200\n",
      "12038/12038 [==============================] - 1s 102us/step - loss: 0.6320 - acc: 0.7864 - val_loss: 0.7957 - val_acc: 0.7687\n",
      "Epoch 144/200\n",
      "12038/12038 [==============================] - 1s 106us/step - loss: 0.6442 - acc: 0.7807 - val_loss: 0.8073 - val_acc: 0.7670\n",
      "Epoch 145/200\n",
      "12038/12038 [==============================] - 1s 104us/step - loss: 0.6404 - acc: 0.7846 - val_loss: 0.8041 - val_acc: 0.7614\n",
      "Epoch 146/200\n",
      "12038/12038 [==============================] - 1s 105us/step - loss: 0.6336 - acc: 0.7878 - val_loss: 0.8032 - val_acc: 0.7611\n",
      "Epoch 147/200\n",
      "12038/12038 [==============================] - 1s 103us/step - loss: 0.6271 - acc: 0.7871 - val_loss: 0.7918 - val_acc: 0.7697\n",
      "Epoch 148/200\n",
      "12038/12038 [==============================] - 1s 102us/step - loss: 0.6182 - acc: 0.7909 - val_loss: 0.8100 - val_acc: 0.7654\n",
      "Epoch 149/200\n",
      "12038/12038 [==============================] - 1s 97us/step - loss: 0.6143 - acc: 0.7936 - val_loss: 0.8190 - val_acc: 0.7657\n",
      "Epoch 150/200\n",
      "12038/12038 [==============================] - 1s 101us/step - loss: 0.6198 - acc: 0.7901 - val_loss: 0.7920 - val_acc: 0.7743\n",
      "Epoch 151/200\n",
      "12038/12038 [==============================] - 1s 100us/step - loss: 0.6220 - acc: 0.7900 - val_loss: 0.8108 - val_acc: 0.7650\n",
      "Epoch 152/200\n",
      "12038/12038 [==============================] - 1s 102us/step - loss: 0.6174 - acc: 0.7947 - val_loss: 0.7969 - val_acc: 0.7657\n",
      "Epoch 153/200\n",
      "12038/12038 [==============================] - 1s 100us/step - loss: 0.6195 - acc: 0.7935 - val_loss: 0.7908 - val_acc: 0.7770\n",
      "Epoch 154/200\n",
      "12038/12038 [==============================] - 1s 104us/step - loss: 0.6131 - acc: 0.7945 - val_loss: 0.7989 - val_acc: 0.7664\n",
      "Epoch 155/200\n",
      "12038/12038 [==============================] - 1s 107us/step - loss: 0.6187 - acc: 0.7893 - val_loss: 0.7905 - val_acc: 0.7737\n",
      "Epoch 156/200\n",
      "12038/12038 [==============================] - 1s 104us/step - loss: 0.6040 - acc: 0.7935 - val_loss: 0.8103 - val_acc: 0.7647\n",
      "Epoch 157/200\n",
      "12038/12038 [==============================] - 1s 100us/step - loss: 0.6040 - acc: 0.7958 - val_loss: 0.8116 - val_acc: 0.7707\n",
      "Epoch 158/200\n",
      "12038/12038 [==============================] - 1s 102us/step - loss: 0.6146 - acc: 0.7872 - val_loss: 0.8085 - val_acc: 0.7674\n",
      "Epoch 159/200\n",
      "12038/12038 [==============================] - 1s 106us/step - loss: 0.6142 - acc: 0.7908 - val_loss: 0.7962 - val_acc: 0.7687\n",
      "Epoch 160/200\n",
      "12038/12038 [==============================] - 1s 108us/step - loss: 0.6216 - acc: 0.7887 - val_loss: 0.7962 - val_acc: 0.7667\n",
      "Epoch 161/200\n",
      "12038/12038 [==============================] - 1s 106us/step - loss: 0.5969 - acc: 0.7981 - val_loss: 0.7875 - val_acc: 0.7823\n",
      "Epoch 162/200\n",
      "12038/12038 [==============================] - 1s 103us/step - loss: 0.5990 - acc: 0.7993 - val_loss: 0.7929 - val_acc: 0.7780\n",
      "Epoch 163/200\n",
      "12038/12038 [==============================] - 1s 97us/step - loss: 0.6183 - acc: 0.7910 - val_loss: 0.7901 - val_acc: 0.7694\n",
      "Epoch 164/200\n",
      "12038/12038 [==============================] - 1s 99us/step - loss: 0.5932 - acc: 0.7947 - val_loss: 0.7980 - val_acc: 0.7710\n",
      "Epoch 165/200\n",
      "12038/12038 [==============================] - 1s 104us/step - loss: 0.5969 - acc: 0.7960 - val_loss: 0.7934 - val_acc: 0.7757\n",
      "Epoch 166/200\n",
      "12038/12038 [==============================] - 1s 101us/step - loss: 0.5868 - acc: 0.7988 - val_loss: 0.7921 - val_acc: 0.7727\n",
      "Epoch 167/200\n",
      "12038/12038 [==============================] - 1s 103us/step - loss: 0.5721 - acc: 0.8016 - val_loss: 0.8047 - val_acc: 0.7654\n",
      "Epoch 168/200\n",
      "12038/12038 [==============================] - 1s 105us/step - loss: 0.5851 - acc: 0.7978 - val_loss: 0.8007 - val_acc: 0.7740\n",
      "Epoch 169/200\n",
      "12038/12038 [==============================] - 1s 101us/step - loss: 0.5836 - acc: 0.8015 - val_loss: 0.8145 - val_acc: 0.7717\n",
      "Epoch 170/200\n",
      "12038/12038 [==============================] - 1s 102us/step - loss: 0.6080 - acc: 0.7967 - val_loss: 0.7812 - val_acc: 0.7803\n",
      "Epoch 171/200\n",
      "12038/12038 [==============================] - 1s 103us/step - loss: 0.5806 - acc: 0.8020 - val_loss: 0.8030 - val_acc: 0.7737\n",
      "Epoch 172/200\n",
      "12038/12038 [==============================] - 1s 102us/step - loss: 0.5925 - acc: 0.7982 - val_loss: 0.8000 - val_acc: 0.7704\n",
      "Epoch 173/200\n",
      "12038/12038 [==============================] - 1s 99us/step - loss: 0.5869 - acc: 0.7994 - val_loss: 0.7928 - val_acc: 0.7760\n",
      "Epoch 174/200\n",
      "12038/12038 [==============================] - 1s 101us/step - loss: 0.5791 - acc: 0.7996 - val_loss: 0.8011 - val_acc: 0.7740\n",
      "Epoch 175/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12038/12038 [==============================] - 1s 108us/step - loss: 0.5701 - acc: 0.8058 - val_loss: 0.8217 - val_acc: 0.7730\n",
      "Epoch 176/200\n",
      "12038/12038 [==============================] - 1s 104us/step - loss: 0.5688 - acc: 0.8079 - val_loss: 0.8141 - val_acc: 0.7694\n",
      "Epoch 177/200\n",
      "12038/12038 [==============================] - 1s 98us/step - loss: 0.5794 - acc: 0.8000 - val_loss: 0.8062 - val_acc: 0.7727\n",
      "Epoch 178/200\n",
      "12038/12038 [==============================] - 1s 99us/step - loss: 0.5756 - acc: 0.8020 - val_loss: 0.8053 - val_acc: 0.7667\n",
      "Epoch 179/200\n",
      "12038/12038 [==============================] - 1s 107us/step - loss: 0.5667 - acc: 0.8054 - val_loss: 0.8153 - val_acc: 0.7707\n",
      "Epoch 180/200\n",
      "12038/12038 [==============================] - 1s 103us/step - loss: 0.5669 - acc: 0.8033 - val_loss: 0.8028 - val_acc: 0.7714\n",
      "Epoch 181/200\n",
      "12038/12038 [==============================] - 1s 102us/step - loss: 0.5745 - acc: 0.8050 - val_loss: 0.8123 - val_acc: 0.7660\n",
      "Epoch 182/200\n",
      "12038/12038 [==============================] - 1s 102us/step - loss: 0.5652 - acc: 0.8079 - val_loss: 0.8048 - val_acc: 0.7714\n",
      "Epoch 183/200\n",
      "12038/12038 [==============================] - 1s 102us/step - loss: 0.5683 - acc: 0.8073 - val_loss: 0.7922 - val_acc: 0.7753\n",
      "Epoch 184/200\n",
      "12038/12038 [==============================] - 1s 103us/step - loss: 0.5479 - acc: 0.8104 - val_loss: 0.7946 - val_acc: 0.7684\n",
      "Epoch 185/200\n",
      "12038/12038 [==============================] - 1s 104us/step - loss: 0.5603 - acc: 0.8092 - val_loss: 0.8406 - val_acc: 0.7674\n",
      "Epoch 186/200\n",
      "12038/12038 [==============================] - 1s 102us/step - loss: 0.5535 - acc: 0.8124 - val_loss: 0.8276 - val_acc: 0.7743\n",
      "Epoch 187/200\n",
      "12038/12038 [==============================] - 1s 99us/step - loss: 0.5575 - acc: 0.8096 - val_loss: 0.8170 - val_acc: 0.7720\n",
      "Epoch 188/200\n",
      "12038/12038 [==============================] - 1s 101us/step - loss: 0.5476 - acc: 0.8104 - val_loss: 0.7998 - val_acc: 0.7787\n",
      "Epoch 189/200\n",
      "12038/12038 [==============================] - 1s 104us/step - loss: 0.5565 - acc: 0.8094 - val_loss: 0.7923 - val_acc: 0.7757\n",
      "Epoch 190/200\n",
      "12038/12038 [==============================] - 1s 104us/step - loss: 0.5497 - acc: 0.8113 - val_loss: 0.8186 - val_acc: 0.7723\n",
      "Epoch 191/200\n",
      "12038/12038 [==============================] - 1s 102us/step - loss: 0.5538 - acc: 0.8140 - val_loss: 0.8107 - val_acc: 0.7667\n",
      "Epoch 192/200\n",
      "12038/12038 [==============================] - 1s 98us/step - loss: 0.5478 - acc: 0.8152 - val_loss: 0.8219 - val_acc: 0.7797\n",
      "Epoch 193/200\n",
      "12038/12038 [==============================] - 1s 102us/step - loss: 0.5515 - acc: 0.8084 - val_loss: 0.8276 - val_acc: 0.7740\n",
      "Epoch 194/200\n",
      "12038/12038 [==============================] - 1s 100us/step - loss: 0.5553 - acc: 0.8095 - val_loss: 0.8235 - val_acc: 0.7740\n",
      "Epoch 195/200\n",
      "12038/12038 [==============================] - 1s 105us/step - loss: 0.5447 - acc: 0.8136 - val_loss: 0.8160 - val_acc: 0.7763\n",
      "Epoch 196/200\n",
      "12038/12038 [==============================] - 1s 103us/step - loss: 0.5456 - acc: 0.8180 - val_loss: 0.8246 - val_acc: 0.7650\n",
      "Epoch 197/200\n",
      "12038/12038 [==============================] - 1s 104us/step - loss: 0.5380 - acc: 0.8157 - val_loss: 0.8211 - val_acc: 0.7757\n",
      "Epoch 198/200\n",
      "12038/12038 [==============================] - 1s 103us/step - loss: 0.5385 - acc: 0.8186 - val_loss: 0.8414 - val_acc: 0.7697\n",
      "Epoch 199/200\n",
      "12038/12038 [==============================] - 1s 103us/step - loss: 0.5492 - acc: 0.8148 - val_loss: 0.8116 - val_acc: 0.7743\n",
      "Epoch 200/200\n",
      "12038/12038 [==============================] - 1s 104us/step - loss: 0.5336 - acc: 0.8181 - val_loss: 0.8257 - val_acc: 0.7733\n",
      "12038/12038 [==============================] - 1s 79us/step\n",
      "Train Loss = 0.26237288919763496\n",
      "Train Accuracy = 0.920667885030736\n",
      "3009/3009 [==============================] - 0s 85us/step\n",
      "Test Loss = 0.8257088054252566\n",
      "Test Accuracy = 0.7733466267863077\n",
      "time: 250.63628101348877\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = LossHistory() # 创建一个history实例\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=200, batch_size=64, verbose=1, \n",
    "            validation_data=(X_test, Y_test),callbacks=[history])\n",
    "\n",
    "preds_train = model.evaluate(X_train, Y_train)\n",
    "print(\"Train Loss = \" + str(preds_train[0]))\n",
    "print(\"Train Accuracy = \" + str(preds_train[1]))\n",
    "\n",
    "preds_test  = model.evaluate(X_test, Y_test)\n",
    "print(\"Test Loss = \" + str(preds_test[0]))\n",
    "print(\"Test Accuracy = \" + str(preds_test[1]))\n",
    "\n",
    "end = time.time()\n",
    "print(\"time:\",end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f45cc8a6d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history.loss_plot('epoch')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
