{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/malele/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (12038, 12, 1, 10)\n",
      "Y_train shape: (12038, 52)\n",
      "X_test shape: (3009, 12, 1, 10)\n",
      "Y_test shape: (3009, 52)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "import keras\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten, Conv2D, Conv1D, MaxPooling2D, concatenate, BatchNormalization\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)].T\n",
    "    return Y\n",
    "\n",
    "file = h5py.File('data/DB1_S1_image.h5','r')\n",
    "imageData   = file['imageData'][:]\n",
    "imageLabel  = file['imageLabel'][:]  \n",
    "file.close()\n",
    "\n",
    "# 随机打乱数据和标签\n",
    "N = imageData.shape[0]\n",
    "index = np.random.permutation(N)\n",
    "data  = imageData[index,:,:]\n",
    "label = imageLabel[index]\n",
    "\n",
    "# 对数据升维,标签one-hot\n",
    "data  = np.expand_dims(data, axis=2)\n",
    "label = convert_to_one_hot(label,52).T\n",
    "\n",
    "# 划分数据集\n",
    "N = data.shape[0]\n",
    "num_train = round(N*0.8)\n",
    "X_train = data[0:num_train,:,:]\n",
    "Y_train = label[0:num_train,:]\n",
    "X_test  = data[num_train:N,:,:]\n",
    "Y_test  = label[num_train:N,:]\n",
    "\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#写一个LossHistory类，保存loss和acc\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = {'batch':[], 'epoch':[]}\n",
    "        self.accuracy = {'batch':[], 'epoch':[]}\n",
    "        self.val_loss = {'batch':[], 'epoch':[]}\n",
    "        self.val_acc = {'batch':[], 'epoch':[]}\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses['batch'].append(logs.get('loss'))\n",
    "        self.accuracy['batch'].append(logs.get('acc'))\n",
    "        self.val_loss['batch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['batch'].append(logs.get('val_acc'))\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses['epoch'].append(logs.get('loss'))\n",
    "        self.accuracy['epoch'].append(logs.get('acc'))\n",
    "        self.val_loss['epoch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['epoch'].append(logs.get('val_acc'))\n",
    "\n",
    "    def loss_plot(self, loss_type):\n",
    "        iters = range(len(self.losses[loss_type]))\n",
    "        plt.figure()\n",
    "        # acc\n",
    "        plt.plot(iters, self.accuracy[loss_type], 'r', label='train acc')\n",
    "        # loss\n",
    "        plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n",
    "        if loss_type == 'epoch':\n",
    "            # val_acc\n",
    "            plt.plot(iters, self.val_acc[loss_type], 'b', label='val acc')\n",
    "            # val_loss\n",
    "            plt.plot(iters, self.val_loss[loss_type], 'k', label='val loss')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(loss_type)\n",
    "        plt.ylabel('acc-loss')\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 12, 1, 10)         0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 12, 1, 32)         1952      \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 12, 1, 64)         8256      \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling2D)         (None, 6, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 6, 1, 128)         16512     \n",
      "_________________________________________________________________\n",
      "pool2 (MaxPooling2D)         (None, 3, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 128)               49280     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 52)                6708      \n",
      "=================================================================\n",
      "Total params: 82,708\n",
      "Trainable params: 82,708\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def CNN(input_shape=(12,1,10), classes=52): \n",
    "    X_input = Input(input_shape)\n",
    "    \n",
    "    X = Conv2D(filters=32,kernel_size=(6,1),strides=(1,1),padding='same',activation='relu',name='conv1')(X_input)\n",
    "    \n",
    "    X = Conv2D(filters=64,kernel_size=(4,1),strides=(1,1),padding='same',activation='relu',name='conv2')(X)\n",
    "    X = MaxPooling2D(pool_size=(2,1), strides=(2,1), name='pool1')(X)\n",
    " \n",
    "    \n",
    "    X = Conv2D(filters=128,kernel_size=(2,1),strides=(1,1),padding='same',activation='relu',name='conv3')(X)\n",
    "    X = MaxPooling2D(pool_size=(2,1), strides=(2,1), name='pool2')(X)\n",
    "    \n",
    "    X = Flatten(name='flatten')(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = Dense(128,activation='relu',name='fc1')(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = Dense(classes, activation='softmax', name='fc2')(X)\n",
    "    \n",
    "    model = Model(inputs=X_input, outputs=X, name='CNN')\n",
    "    return model\n",
    "model = CNN(input_shape = (12, 1, 10), classes = 52)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练原始数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12038 samples, validate on 3009 samples\n",
      "Epoch 1/200\n",
      "12038/12038 [==============================] - 3s 212us/step - loss: 3.5444 - acc: 0.0920 - val_loss: 2.8406 - val_acc: 0.2742\n",
      "Epoch 2/200\n",
      "12038/12038 [==============================] - 2s 159us/step - loss: 2.8008 - acc: 0.2394 - val_loss: 2.3414 - val_acc: 0.3662\n",
      "Epoch 3/200\n",
      "12038/12038 [==============================] - 2s 159us/step - loss: 2.4384 - acc: 0.3155 - val_loss: 2.0880 - val_acc: 0.4307\n",
      "Epoch 4/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 2.2279 - acc: 0.3772 - val_loss: 1.8996 - val_acc: 0.4812\n",
      "Epoch 5/200\n",
      "12038/12038 [==============================] - 2s 161us/step - loss: 2.0547 - acc: 0.4176 - val_loss: 1.7963 - val_acc: 0.4922\n",
      "Epoch 6/200\n",
      "12038/12038 [==============================] - 2s 161us/step - loss: 1.9326 - acc: 0.4495 - val_loss: 1.6601 - val_acc: 0.5291\n",
      "Epoch 7/200\n",
      "12038/12038 [==============================] - 2s 159us/step - loss: 1.8410 - acc: 0.4742 - val_loss: 1.5899 - val_acc: 0.5487\n",
      "Epoch 8/200\n",
      "12038/12038 [==============================] - 2s 159us/step - loss: 1.7625 - acc: 0.4911 - val_loss: 1.5290 - val_acc: 0.5666\n",
      "Epoch 9/200\n",
      "12038/12038 [==============================] - 2s 162us/step - loss: 1.6940 - acc: 0.5100 - val_loss: 1.4692 - val_acc: 0.5779\n",
      "Epoch 10/200\n",
      "12038/12038 [==============================] - 2s 161us/step - loss: 1.6257 - acc: 0.5248 - val_loss: 1.4215 - val_acc: 0.5852\n",
      "Epoch 11/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 1.5973 - acc: 0.5390 - val_loss: 1.4018 - val_acc: 0.5849\n",
      "Epoch 12/200\n",
      "12038/12038 [==============================] - 2s 161us/step - loss: 1.5419 - acc: 0.5454 - val_loss: 1.3442 - val_acc: 0.5989\n",
      "Epoch 13/200\n",
      "12038/12038 [==============================] - 2s 163us/step - loss: 1.4892 - acc: 0.5593 - val_loss: 1.3535 - val_acc: 0.5932\n",
      "Epoch 14/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 1.4661 - acc: 0.5644 - val_loss: 1.2962 - val_acc: 0.6148\n",
      "Epoch 15/200\n",
      "12038/12038 [==============================] - 2s 158us/step - loss: 1.4310 - acc: 0.5775 - val_loss: 1.2582 - val_acc: 0.6228\n",
      "Epoch 16/200\n",
      "12038/12038 [==============================] - 2s 161us/step - loss: 1.3952 - acc: 0.5856 - val_loss: 1.2385 - val_acc: 0.6278\n",
      "Epoch 17/200\n",
      "12038/12038 [==============================] - 2s 162us/step - loss: 1.3790 - acc: 0.5906 - val_loss: 1.2422 - val_acc: 0.6288\n",
      "Epoch 18/200\n",
      "12038/12038 [==============================] - 2s 161us/step - loss: 1.3399 - acc: 0.5993 - val_loss: 1.2040 - val_acc: 0.6394\n",
      "Epoch 19/200\n",
      "12038/12038 [==============================] - 2s 161us/step - loss: 1.3438 - acc: 0.5989 - val_loss: 1.1992 - val_acc: 0.6371\n",
      "Epoch 20/200\n",
      "12038/12038 [==============================] - 2s 163us/step - loss: 1.2987 - acc: 0.6109 - val_loss: 1.1872 - val_acc: 0.6434\n",
      "Epoch 21/200\n",
      "12038/12038 [==============================] - 2s 162us/step - loss: 1.2912 - acc: 0.6093 - val_loss: 1.1672 - val_acc: 0.6491\n",
      "Epoch 22/200\n",
      "12038/12038 [==============================] - 2s 162us/step - loss: 1.2927 - acc: 0.6093 - val_loss: 1.1566 - val_acc: 0.6481\n",
      "Epoch 23/200\n",
      "12038/12038 [==============================] - 2s 162us/step - loss: 1.2562 - acc: 0.6192 - val_loss: 1.1378 - val_acc: 0.6560\n",
      "Epoch 24/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 1.2416 - acc: 0.6244 - val_loss: 1.1341 - val_acc: 0.6597\n",
      "Epoch 25/200\n",
      "12038/12038 [==============================] - 2s 162us/step - loss: 1.2291 - acc: 0.6298 - val_loss: 1.1452 - val_acc: 0.6491\n",
      "Epoch 26/200\n",
      "12038/12038 [==============================] - 2s 159us/step - loss: 1.1989 - acc: 0.6318 - val_loss: 1.1192 - val_acc: 0.6683\n",
      "Epoch 27/200\n",
      "12038/12038 [==============================] - 2s 161us/step - loss: 1.1877 - acc: 0.6411 - val_loss: 1.1001 - val_acc: 0.6677\n",
      "Epoch 28/200\n",
      "12038/12038 [==============================] - 2s 162us/step - loss: 1.1732 - acc: 0.6336 - val_loss: 1.0771 - val_acc: 0.6743\n",
      "Epoch 29/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 1.1600 - acc: 0.6450 - val_loss: 1.1127 - val_acc: 0.6650\n",
      "Epoch 30/200\n",
      "12038/12038 [==============================] - 2s 162us/step - loss: 1.1455 - acc: 0.6456 - val_loss: 1.0814 - val_acc: 0.6693\n",
      "Epoch 31/200\n",
      "12038/12038 [==============================] - 2s 161us/step - loss: 1.1540 - acc: 0.6450 - val_loss: 1.0680 - val_acc: 0.6707\n",
      "Epoch 32/200\n",
      "12038/12038 [==============================] - 2s 159us/step - loss: 1.1440 - acc: 0.6476 - val_loss: 1.0860 - val_acc: 0.6766\n",
      "Epoch 33/200\n",
      "12038/12038 [==============================] - 2s 162us/step - loss: 1.1211 - acc: 0.6570 - val_loss: 1.0629 - val_acc: 0.6766\n",
      "Epoch 34/200\n",
      "12038/12038 [==============================] - 2s 162us/step - loss: 1.1131 - acc: 0.6553 - val_loss: 1.0596 - val_acc: 0.6810\n",
      "Epoch 35/200\n",
      "12038/12038 [==============================] - 2s 162us/step - loss: 1.1039 - acc: 0.6599 - val_loss: 1.0616 - val_acc: 0.6780\n",
      "Epoch 36/200\n",
      "12038/12038 [==============================] - 2s 161us/step - loss: 1.0891 - acc: 0.6632 - val_loss: 1.0539 - val_acc: 0.6773\n",
      "Epoch 37/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 1.0847 - acc: 0.6611 - val_loss: 1.0407 - val_acc: 0.6763\n",
      "Epoch 38/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 1.0749 - acc: 0.6666 - val_loss: 1.0499 - val_acc: 0.6800\n",
      "Epoch 39/200\n",
      "12038/12038 [==============================] - 2s 161us/step - loss: 1.0773 - acc: 0.6682 - val_loss: 1.0398 - val_acc: 0.6873\n",
      "Epoch 40/200\n",
      "12038/12038 [==============================] - 2s 161us/step - loss: 1.0517 - acc: 0.6692 - val_loss: 1.0293 - val_acc: 0.6906\n",
      "Epoch 41/200\n",
      "12038/12038 [==============================] - 2s 161us/step - loss: 1.0585 - acc: 0.6698 - val_loss: 1.0356 - val_acc: 0.6786\n",
      "Epoch 42/200\n",
      "12038/12038 [==============================] - 2s 161us/step - loss: 1.0337 - acc: 0.6803 - val_loss: 1.0289 - val_acc: 0.6790\n",
      "Epoch 43/200\n",
      "12038/12038 [==============================] - 2s 161us/step - loss: 1.0271 - acc: 0.6825 - val_loss: 1.0107 - val_acc: 0.6823\n",
      "Epoch 44/200\n",
      "12038/12038 [==============================] - 2s 159us/step - loss: 1.0227 - acc: 0.6774 - val_loss: 1.0250 - val_acc: 0.6893\n",
      "Epoch 45/200\n",
      "12038/12038 [==============================] - 2s 161us/step - loss: 1.0161 - acc: 0.6851 - val_loss: 1.0130 - val_acc: 0.6929\n",
      "Epoch 46/200\n",
      "12038/12038 [==============================] - 2s 161us/step - loss: 1.0122 - acc: 0.6794 - val_loss: 1.0164 - val_acc: 0.6893\n",
      "Epoch 47/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 1.0242 - acc: 0.6798 - val_loss: 1.0238 - val_acc: 0.6843\n",
      "Epoch 48/200\n",
      "12038/12038 [==============================] - 2s 159us/step - loss: 1.0028 - acc: 0.6896 - val_loss: 0.9888 - val_acc: 0.6909\n",
      "Epoch 49/200\n",
      "12038/12038 [==============================] - 2s 162us/step - loss: 1.0034 - acc: 0.6853 - val_loss: 0.9908 - val_acc: 0.7032\n",
      "Epoch 50/200\n",
      "12038/12038 [==============================] - 2s 162us/step - loss: 1.0023 - acc: 0.6849 - val_loss: 1.0079 - val_acc: 0.6836\n",
      "Epoch 51/200\n",
      "12038/12038 [==============================] - 2s 163us/step - loss: 0.9811 - acc: 0.6919 - val_loss: 0.9847 - val_acc: 0.6992\n",
      "Epoch 52/200\n",
      "12038/12038 [==============================] - 2s 162us/step - loss: 0.9728 - acc: 0.6915 - val_loss: 0.9782 - val_acc: 0.6896\n",
      "Epoch 53/200\n",
      "12038/12038 [==============================] - 2s 162us/step - loss: 0.9666 - acc: 0.6962 - val_loss: 0.9861 - val_acc: 0.7026\n",
      "Epoch 54/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.9720 - acc: 0.6977 - val_loss: 0.9824 - val_acc: 0.6939\n",
      "Epoch 55/200\n",
      "12038/12038 [==============================] - 2s 161us/step - loss: 0.9678 - acc: 0.7005 - val_loss: 0.9638 - val_acc: 0.7072\n",
      "Epoch 56/200\n",
      "12038/12038 [==============================] - 2s 161us/step - loss: 0.9491 - acc: 0.7039 - val_loss: 0.9645 - val_acc: 0.7085\n",
      "Epoch 57/200\n",
      "12038/12038 [==============================] - 2s 158us/step - loss: 0.9623 - acc: 0.6938 - val_loss: 0.9616 - val_acc: 0.7036\n",
      "Epoch 58/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.9403 - acc: 0.7087 - val_loss: 0.9787 - val_acc: 0.7022\n",
      "Epoch 59/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12038/12038 [==============================] - 2s 162us/step - loss: 0.9477 - acc: 0.7031 - val_loss: 0.9717 - val_acc: 0.7009\n",
      "Epoch 60/200\n",
      "12038/12038 [==============================] - 2s 162us/step - loss: 0.9339 - acc: 0.7037 - val_loss: 0.9607 - val_acc: 0.7059\n",
      "Epoch 61/200\n",
      "12038/12038 [==============================] - 2s 162us/step - loss: 0.9443 - acc: 0.6984 - val_loss: 0.9761 - val_acc: 0.7032\n",
      "Epoch 62/200\n",
      "12038/12038 [==============================] - 2s 159us/step - loss: 0.9182 - acc: 0.7104 - val_loss: 0.9570 - val_acc: 0.7036\n",
      "Epoch 63/200\n",
      "12038/12038 [==============================] - 2s 161us/step - loss: 0.9262 - acc: 0.7068 - val_loss: 0.9776 - val_acc: 0.7079\n",
      "Epoch 64/200\n",
      "12038/12038 [==============================] - 2s 159us/step - loss: 0.9163 - acc: 0.7067 - val_loss: 0.9722 - val_acc: 0.7036\n",
      "Epoch 65/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.9307 - acc: 0.7079 - val_loss: 0.9649 - val_acc: 0.7036\n",
      "Epoch 66/200\n",
      "12038/12038 [==============================] - 2s 163us/step - loss: 0.9243 - acc: 0.7120 - val_loss: 0.9800 - val_acc: 0.6989\n",
      "Epoch 67/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.9060 - acc: 0.7137 - val_loss: 0.9670 - val_acc: 0.7089\n",
      "Epoch 68/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.9097 - acc: 0.7119 - val_loss: 0.9514 - val_acc: 0.7105\n",
      "Epoch 69/200\n",
      "12038/12038 [==============================] - 2s 162us/step - loss: 0.9026 - acc: 0.7161 - val_loss: 0.9642 - val_acc: 0.7052\n",
      "Epoch 70/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.9249 - acc: 0.7090 - val_loss: 0.9590 - val_acc: 0.7052\n",
      "Epoch 71/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.9035 - acc: 0.7105 - val_loss: 0.9780 - val_acc: 0.7016\n",
      "Epoch 72/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.8857 - acc: 0.7204 - val_loss: 0.9732 - val_acc: 0.7026\n",
      "Epoch 73/200\n",
      "12038/12038 [==============================] - 2s 158us/step - loss: 0.8874 - acc: 0.7204 - val_loss: 0.9465 - val_acc: 0.7049\n",
      "Epoch 74/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.8912 - acc: 0.7117 - val_loss: 0.9572 - val_acc: 0.7089\n",
      "Epoch 75/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.8913 - acc: 0.7136 - val_loss: 0.9534 - val_acc: 0.7149\n",
      "Epoch 76/200\n",
      "12038/12038 [==============================] - 2s 162us/step - loss: 0.9003 - acc: 0.7112 - val_loss: 0.9510 - val_acc: 0.7082\n",
      "Epoch 77/200\n",
      "12038/12038 [==============================] - 2s 162us/step - loss: 0.8723 - acc: 0.7222 - val_loss: 0.9401 - val_acc: 0.7135\n",
      "Epoch 78/200\n",
      "12038/12038 [==============================] - 2s 161us/step - loss: 0.8667 - acc: 0.7242 - val_loss: 0.9537 - val_acc: 0.7119\n",
      "Epoch 79/200\n",
      "12038/12038 [==============================] - 2s 159us/step - loss: 0.8707 - acc: 0.7253 - val_loss: 0.9437 - val_acc: 0.7165\n",
      "Epoch 80/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.8599 - acc: 0.7261 - val_loss: 0.9506 - val_acc: 0.7132\n",
      "Epoch 81/200\n",
      "12038/12038 [==============================] - 2s 162us/step - loss: 0.8569 - acc: 0.7301 - val_loss: 0.9289 - val_acc: 0.7205\n",
      "Epoch 82/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.8628 - acc: 0.7230 - val_loss: 0.9422 - val_acc: 0.7142\n",
      "Epoch 83/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.8727 - acc: 0.7186 - val_loss: 0.9308 - val_acc: 0.7192\n",
      "Epoch 84/200\n",
      "12038/12038 [==============================] - 2s 162us/step - loss: 0.8543 - acc: 0.7301 - val_loss: 0.9423 - val_acc: 0.7132\n",
      "Epoch 85/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.8411 - acc: 0.7273 - val_loss: 0.9635 - val_acc: 0.7135\n",
      "Epoch 86/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.8443 - acc: 0.7350 - val_loss: 0.9436 - val_acc: 0.7162\n",
      "Epoch 87/200\n",
      "12038/12038 [==============================] - 2s 162us/step - loss: 0.8502 - acc: 0.7314 - val_loss: 0.9499 - val_acc: 0.7075\n",
      "Epoch 88/200\n",
      "12038/12038 [==============================] - 2s 159us/step - loss: 0.8304 - acc: 0.7351 - val_loss: 0.9544 - val_acc: 0.7102\n",
      "Epoch 89/200\n",
      "12038/12038 [==============================] - 2s 159us/step - loss: 0.8470 - acc: 0.7276 - val_loss: 0.9447 - val_acc: 0.7142\n",
      "Epoch 90/200\n",
      "12038/12038 [==============================] - 2s 162us/step - loss: 0.8463 - acc: 0.7286 - val_loss: 0.9577 - val_acc: 0.7069\n",
      "Epoch 91/200\n",
      "12038/12038 [==============================] - 2s 157us/step - loss: 0.8207 - acc: 0.7358 - val_loss: 0.9619 - val_acc: 0.7075\n",
      "Epoch 92/200\n",
      "12038/12038 [==============================] - 2s 161us/step - loss: 0.8287 - acc: 0.7328 - val_loss: 0.9226 - val_acc: 0.7202\n",
      "Epoch 93/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.8227 - acc: 0.7385 - val_loss: 0.9323 - val_acc: 0.7212\n",
      "Epoch 94/200\n",
      "12038/12038 [==============================] - 2s 161us/step - loss: 0.8227 - acc: 0.7397 - val_loss: 0.9495 - val_acc: 0.7105\n",
      "Epoch 95/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.8212 - acc: 0.7350 - val_loss: 0.9425 - val_acc: 0.7212\n",
      "Epoch 96/200\n",
      "12038/12038 [==============================] - 2s 161us/step - loss: 0.8368 - acc: 0.7327 - val_loss: 0.9396 - val_acc: 0.7215\n",
      "Epoch 97/200\n",
      "12038/12038 [==============================] - 2s 157us/step - loss: 0.8082 - acc: 0.7391 - val_loss: 0.9464 - val_acc: 0.7155\n",
      "Epoch 98/200\n",
      "12038/12038 [==============================] - 2s 159us/step - loss: 0.8015 - acc: 0.7458 - val_loss: 0.9351 - val_acc: 0.7208\n",
      "Epoch 99/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.8146 - acc: 0.7363 - val_loss: 0.9278 - val_acc: 0.7198\n",
      "Epoch 100/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.8223 - acc: 0.7342 - val_loss: 0.9425 - val_acc: 0.7215\n",
      "Epoch 101/200\n",
      "12038/12038 [==============================] - 2s 158us/step - loss: 0.8079 - acc: 0.7424 - val_loss: 0.9328 - val_acc: 0.7172\n",
      "Epoch 102/200\n",
      "12038/12038 [==============================] - 2s 162us/step - loss: 0.7961 - acc: 0.7432 - val_loss: 0.9189 - val_acc: 0.7195\n",
      "Epoch 103/200\n",
      "12038/12038 [==============================] - 2s 159us/step - loss: 0.7945 - acc: 0.7392 - val_loss: 0.9401 - val_acc: 0.7235\n",
      "Epoch 104/200\n",
      "12038/12038 [==============================] - 2s 159us/step - loss: 0.7838 - acc: 0.7495 - val_loss: 0.9397 - val_acc: 0.7202\n",
      "Epoch 105/200\n",
      "12038/12038 [==============================] - 2s 163us/step - loss: 0.8135 - acc: 0.7389 - val_loss: 0.9401 - val_acc: 0.7188\n",
      "Epoch 106/200\n",
      "12038/12038 [==============================] - 2s 159us/step - loss: 0.8067 - acc: 0.7397 - val_loss: 0.9344 - val_acc: 0.7218\n",
      "Epoch 107/200\n",
      "12038/12038 [==============================] - 2s 162us/step - loss: 0.7812 - acc: 0.7462 - val_loss: 0.9184 - val_acc: 0.7235\n",
      "Epoch 108/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.7979 - acc: 0.7420 - val_loss: 0.9279 - val_acc: 0.7172\n",
      "Epoch 109/200\n",
      "12038/12038 [==============================] - 2s 164us/step - loss: 0.7805 - acc: 0.7500 - val_loss: 0.9376 - val_acc: 0.7178\n",
      "Epoch 110/200\n",
      "12038/12038 [==============================] - 2s 161us/step - loss: 0.8007 - acc: 0.7466 - val_loss: 0.9420 - val_acc: 0.7149\n",
      "Epoch 111/200\n",
      "12038/12038 [==============================] - 2s 161us/step - loss: 0.7800 - acc: 0.7485 - val_loss: 0.9271 - val_acc: 0.7228\n",
      "Epoch 112/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.8122 - acc: 0.7429 - val_loss: 0.9232 - val_acc: 0.7142\n",
      "Epoch 113/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.7860 - acc: 0.7454 - val_loss: 0.9392 - val_acc: 0.7159\n",
      "Epoch 114/200\n",
      "12038/12038 [==============================] - 2s 158us/step - loss: 0.8014 - acc: 0.7404 - val_loss: 0.9220 - val_acc: 0.7275\n",
      "Epoch 115/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.7793 - acc: 0.7498 - val_loss: 0.9296 - val_acc: 0.7152\n",
      "Epoch 116/200\n",
      "12038/12038 [==============================] - 2s 162us/step - loss: 0.7894 - acc: 0.7453 - val_loss: 0.9336 - val_acc: 0.7235\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12038/12038 [==============================] - 2s 161us/step - loss: 0.7617 - acc: 0.7505 - val_loss: 0.9287 - val_acc: 0.7252\n",
      "Epoch 118/200\n",
      "12038/12038 [==============================] - 2s 161us/step - loss: 0.7683 - acc: 0.7531 - val_loss: 0.9141 - val_acc: 0.7288\n",
      "Epoch 119/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.7844 - acc: 0.7468 - val_loss: 0.9258 - val_acc: 0.7288\n",
      "Epoch 120/200\n",
      "12038/12038 [==============================] - 2s 162us/step - loss: 0.7663 - acc: 0.7521 - val_loss: 0.9248 - val_acc: 0.7335\n",
      "Epoch 121/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.7580 - acc: 0.7520 - val_loss: 0.9318 - val_acc: 0.7202\n",
      "Epoch 122/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.7569 - acc: 0.7585 - val_loss: 0.9241 - val_acc: 0.7228\n",
      "Epoch 123/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.7612 - acc: 0.7534 - val_loss: 0.9465 - val_acc: 0.7222\n",
      "Epoch 124/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.7573 - acc: 0.7567 - val_loss: 0.9167 - val_acc: 0.7255\n",
      "Epoch 125/200\n",
      "12038/12038 [==============================] - 2s 159us/step - loss: 0.7406 - acc: 0.7595 - val_loss: 0.9241 - val_acc: 0.7265\n",
      "Epoch 126/200\n",
      "12038/12038 [==============================] - 2s 158us/step - loss: 0.7622 - acc: 0.7534 - val_loss: 0.9351 - val_acc: 0.7291\n",
      "Epoch 127/200\n",
      "12038/12038 [==============================] - 2s 161us/step - loss: 0.7471 - acc: 0.7545 - val_loss: 0.9274 - val_acc: 0.7272\n",
      "Epoch 128/200\n",
      "12038/12038 [==============================] - 2s 161us/step - loss: 0.7340 - acc: 0.7595 - val_loss: 0.9446 - val_acc: 0.7205\n",
      "Epoch 129/200\n",
      "12038/12038 [==============================] - 2s 159us/step - loss: 0.7444 - acc: 0.7632 - val_loss: 0.9099 - val_acc: 0.7348\n",
      "Epoch 130/200\n",
      "12038/12038 [==============================] - 2s 161us/step - loss: 0.7444 - acc: 0.7573 - val_loss: 0.9276 - val_acc: 0.7252\n",
      "Epoch 131/200\n",
      "12038/12038 [==============================] - 2s 158us/step - loss: 0.7483 - acc: 0.7564 - val_loss: 0.9242 - val_acc: 0.7301\n",
      "Epoch 132/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.7492 - acc: 0.7581 - val_loss: 0.9008 - val_acc: 0.7338\n",
      "Epoch 133/200\n",
      "12038/12038 [==============================] - 2s 159us/step - loss: 0.7399 - acc: 0.7605 - val_loss: 0.9435 - val_acc: 0.7311\n",
      "Epoch 134/200\n",
      "12038/12038 [==============================] - 2s 159us/step - loss: 0.7365 - acc: 0.7607 - val_loss: 0.9148 - val_acc: 0.7361\n",
      "Epoch 135/200\n",
      "12038/12038 [==============================] - 2s 158us/step - loss: 0.7460 - acc: 0.7598 - val_loss: 0.9414 - val_acc: 0.7258\n",
      "Epoch 136/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.7358 - acc: 0.7647 - val_loss: 0.9383 - val_acc: 0.7248\n",
      "Epoch 137/200\n",
      "12038/12038 [==============================] - 2s 161us/step - loss: 0.7555 - acc: 0.7550 - val_loss: 0.9063 - val_acc: 0.7268\n",
      "Epoch 138/200\n",
      "12038/12038 [==============================] - 2s 159us/step - loss: 0.7401 - acc: 0.7597 - val_loss: 0.9515 - val_acc: 0.7258\n",
      "Epoch 139/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.7578 - acc: 0.7600 - val_loss: 0.9283 - val_acc: 0.7291\n",
      "Epoch 140/200\n",
      "12038/12038 [==============================] - 2s 159us/step - loss: 0.7242 - acc: 0.7656 - val_loss: 0.9232 - val_acc: 0.7321\n",
      "Epoch 141/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.7389 - acc: 0.7637 - val_loss: 0.9026 - val_acc: 0.7361\n",
      "Epoch 142/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.7338 - acc: 0.7632 - val_loss: 0.9546 - val_acc: 0.7295\n",
      "Epoch 143/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.7374 - acc: 0.7603 - val_loss: 0.9324 - val_acc: 0.7318\n",
      "Epoch 144/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.7374 - acc: 0.7603 - val_loss: 0.9275 - val_acc: 0.7215\n",
      "Epoch 145/200\n",
      "12038/12038 [==============================] - 2s 162us/step - loss: 0.7067 - acc: 0.7682 - val_loss: 0.9447 - val_acc: 0.7275\n",
      "Epoch 146/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.7316 - acc: 0.7632 - val_loss: 0.9292 - val_acc: 0.7245\n",
      "Epoch 147/200\n",
      "12038/12038 [==============================] - 2s 161us/step - loss: 0.7232 - acc: 0.7672 - val_loss: 0.9260 - val_acc: 0.7305\n",
      "Epoch 148/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.7255 - acc: 0.7671 - val_loss: 0.9473 - val_acc: 0.7228\n",
      "Epoch 149/200\n",
      "12038/12038 [==============================] - 2s 162us/step - loss: 0.7159 - acc: 0.7682 - val_loss: 0.9349 - val_acc: 0.7272\n",
      "Epoch 150/200\n",
      "12038/12038 [==============================] - 2s 159us/step - loss: 0.7243 - acc: 0.7636 - val_loss: 0.9089 - val_acc: 0.7285\n",
      "Epoch 151/200\n",
      "12038/12038 [==============================] - 2s 163us/step - loss: 0.7141 - acc: 0.7652 - val_loss: 0.9334 - val_acc: 0.7272\n",
      "Epoch 152/200\n",
      "12038/12038 [==============================] - 2s 161us/step - loss: 0.7080 - acc: 0.7681 - val_loss: 0.9120 - val_acc: 0.7318\n",
      "Epoch 153/200\n",
      "12038/12038 [==============================] - 2s 161us/step - loss: 0.7448 - acc: 0.7603 - val_loss: 0.9227 - val_acc: 0.7318\n",
      "Epoch 154/200\n",
      "12038/12038 [==============================] - 2s 162us/step - loss: 0.7252 - acc: 0.7648 - val_loss: 0.9313 - val_acc: 0.7242\n",
      "Epoch 155/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.7147 - acc: 0.7669 - val_loss: 0.9440 - val_acc: 0.7268\n",
      "Epoch 156/200\n",
      "12038/12038 [==============================] - 2s 158us/step - loss: 0.7113 - acc: 0.7715 - val_loss: 0.9293 - val_acc: 0.7288\n",
      "Epoch 157/200\n",
      "12038/12038 [==============================] - 2s 161us/step - loss: 0.6967 - acc: 0.7754 - val_loss: 0.9536 - val_acc: 0.7152\n",
      "Epoch 158/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.7230 - acc: 0.7637 - val_loss: 0.9188 - val_acc: 0.7318\n",
      "Epoch 159/200\n",
      "12038/12038 [==============================] - 2s 161us/step - loss: 0.6886 - acc: 0.7748 - val_loss: 0.9213 - val_acc: 0.7338\n",
      "Epoch 160/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.6976 - acc: 0.7733 - val_loss: 0.9251 - val_acc: 0.7238\n",
      "Epoch 161/200\n",
      "12038/12038 [==============================] - 2s 159us/step - loss: 0.7082 - acc: 0.7682 - val_loss: 0.9320 - val_acc: 0.7262\n",
      "Epoch 162/200\n",
      "12038/12038 [==============================] - 2s 159us/step - loss: 0.6961 - acc: 0.7704 - val_loss: 0.9234 - val_acc: 0.7278\n",
      "Epoch 163/200\n",
      "12038/12038 [==============================] - 2s 163us/step - loss: 0.7158 - acc: 0.7707 - val_loss: 0.9397 - val_acc: 0.7341\n",
      "Epoch 164/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.6882 - acc: 0.7764 - val_loss: 0.9132 - val_acc: 0.7281\n",
      "Epoch 165/200\n",
      "12038/12038 [==============================] - 2s 161us/step - loss: 0.7074 - acc: 0.7682 - val_loss: 0.9295 - val_acc: 0.7268\n",
      "Epoch 166/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.6947 - acc: 0.7682 - val_loss: 0.9322 - val_acc: 0.7281\n",
      "Epoch 167/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.7035 - acc: 0.7740 - val_loss: 0.9344 - val_acc: 0.7365\n",
      "Epoch 168/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.6919 - acc: 0.7741 - val_loss: 0.9309 - val_acc: 0.7351\n",
      "Epoch 169/200\n",
      "12038/12038 [==============================] - 2s 161us/step - loss: 0.6961 - acc: 0.7723 - val_loss: 0.9294 - val_acc: 0.7318\n",
      "Epoch 170/200\n",
      "12038/12038 [==============================] - 2s 159us/step - loss: 0.6978 - acc: 0.7729 - val_loss: 0.9042 - val_acc: 0.7448\n",
      "Epoch 171/200\n",
      "12038/12038 [==============================] - 2s 159us/step - loss: 0.6972 - acc: 0.7732 - val_loss: 0.9221 - val_acc: 0.7281\n",
      "Epoch 172/200\n",
      "12038/12038 [==============================] - 2s 161us/step - loss: 0.6934 - acc: 0.7738 - val_loss: 0.9331 - val_acc: 0.7255\n",
      "Epoch 173/200\n",
      "12038/12038 [==============================] - 2s 159us/step - loss: 0.6988 - acc: 0.7730 - val_loss: 0.9242 - val_acc: 0.7298\n",
      "Epoch 174/200\n",
      "12038/12038 [==============================] - 2s 159us/step - loss: 0.6893 - acc: 0.7762 - val_loss: 0.9173 - val_acc: 0.7328\n",
      "Epoch 175/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.7032 - acc: 0.7698 - val_loss: 0.9499 - val_acc: 0.7228\n",
      "Epoch 176/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.6839 - acc: 0.7755 - val_loss: 0.9247 - val_acc: 0.7388\n",
      "Epoch 177/200\n",
      "12038/12038 [==============================] - 2s 158us/step - loss: 0.7035 - acc: 0.7711 - val_loss: 0.9290 - val_acc: 0.7308\n",
      "Epoch 178/200\n",
      "12038/12038 [==============================] - 2s 159us/step - loss: 0.6905 - acc: 0.7734 - val_loss: 0.9122 - val_acc: 0.7321\n",
      "Epoch 179/200\n",
      "12038/12038 [==============================] - 2s 158us/step - loss: 0.6802 - acc: 0.7814 - val_loss: 0.9084 - val_acc: 0.7378\n",
      "Epoch 180/200\n",
      "12038/12038 [==============================] - 2s 159us/step - loss: 0.6945 - acc: 0.7746 - val_loss: 0.9435 - val_acc: 0.7325\n",
      "Epoch 181/200\n",
      "12038/12038 [==============================] - 2s 161us/step - loss: 0.7043 - acc: 0.7692 - val_loss: 0.9168 - val_acc: 0.7285\n",
      "Epoch 182/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.6750 - acc: 0.7770 - val_loss: 0.9224 - val_acc: 0.7298\n",
      "Epoch 183/200\n",
      "12038/12038 [==============================] - 2s 159us/step - loss: 0.6869 - acc: 0.7786 - val_loss: 0.9543 - val_acc: 0.7218\n",
      "Epoch 184/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.6850 - acc: 0.7764 - val_loss: 0.9373 - val_acc: 0.7358\n",
      "Epoch 185/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.6902 - acc: 0.7780 - val_loss: 0.9452 - val_acc: 0.7275\n",
      "Epoch 186/200\n",
      "12038/12038 [==============================] - 2s 159us/step - loss: 0.6844 - acc: 0.7775 - val_loss: 0.9331 - val_acc: 0.7335\n",
      "Epoch 187/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.6888 - acc: 0.7737 - val_loss: 0.9395 - val_acc: 0.7321\n",
      "Epoch 188/200\n",
      "12038/12038 [==============================] - 2s 157us/step - loss: 0.6798 - acc: 0.7762 - val_loss: 0.9356 - val_acc: 0.7311\n",
      "Epoch 189/200\n",
      "12038/12038 [==============================] - 2s 162us/step - loss: 0.6676 - acc: 0.7793 - val_loss: 0.9266 - val_acc: 0.7321\n",
      "Epoch 190/200\n",
      "12038/12038 [==============================] - 2s 163us/step - loss: 0.6775 - acc: 0.7822 - val_loss: 0.9437 - val_acc: 0.7285\n",
      "Epoch 191/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.6748 - acc: 0.7812 - val_loss: 0.9397 - val_acc: 0.7325\n",
      "Epoch 192/200\n",
      "12038/12038 [==============================] - 2s 159us/step - loss: 0.6602 - acc: 0.7814 - val_loss: 0.9379 - val_acc: 0.7331\n",
      "Epoch 193/200\n",
      "12038/12038 [==============================] - 2s 159us/step - loss: 0.6747 - acc: 0.7785 - val_loss: 0.9359 - val_acc: 0.7438\n",
      "Epoch 194/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.6763 - acc: 0.7740 - val_loss: 0.9247 - val_acc: 0.7311\n",
      "Epoch 195/200\n",
      "12038/12038 [==============================] - 2s 159us/step - loss: 0.6682 - acc: 0.7808 - val_loss: 0.9320 - val_acc: 0.7385\n",
      "Epoch 196/200\n",
      "12038/12038 [==============================] - 2s 161us/step - loss: 0.6572 - acc: 0.7804 - val_loss: 0.9502 - val_acc: 0.7338\n",
      "Epoch 197/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.6560 - acc: 0.7850 - val_loss: 0.9427 - val_acc: 0.7388\n",
      "Epoch 198/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.6721 - acc: 0.7823 - val_loss: 0.9676 - val_acc: 0.7355\n",
      "Epoch 199/200\n",
      "12038/12038 [==============================] - 2s 161us/step - loss: 0.6731 - acc: 0.7789 - val_loss: 0.9578 - val_acc: 0.7351\n",
      "Epoch 200/200\n",
      "12038/12038 [==============================] - 2s 160us/step - loss: 0.6613 - acc: 0.7782 - val_loss: 0.9437 - val_acc: 0.7325\n",
      "12038/12038 [==============================] - 1s 93us/step\n",
      "Train Loss = 0.34720873135053987\n",
      "Train Accuracy = 0.8849476657351063\n",
      "3009/3009 [==============================] - 0s 96us/step\n",
      "Test Loss = 0.9437247829486382\n",
      "Test Accuracy = 0.7324692588899967\n",
      "time: 389.19156980514526\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = LossHistory() # 创建一个history实例\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=200, batch_size=64, verbose=1, \n",
    "            validation_data=(X_test, Y_test),callbacks=[history])\n",
    "\n",
    "preds_train = model.evaluate(X_train, Y_train)\n",
    "print(\"Train Loss = \" + str(preds_train[0]))\n",
    "print(\"Train Accuracy = \" + str(preds_train[1]))\n",
    "\n",
    "preds_test  = model.evaluate(X_test, Y_test)\n",
    "print(\"Test Loss = \" + str(preds_test[0]))\n",
    "print(\"Test Accuracy = \" + str(preds_test[1]))\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print(\"time:\",end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1c2611f080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history.loss_plot('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
