{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/malele/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (12038, 12, 10, 1)\n",
      "Y_train shape: (12038, 52)\n",
      "X_test shape: (3009, 12, 10, 1)\n",
      "Y_test shape: (3009, 52)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "import keras\n",
    "from keras.layers import Input, Dense, ZeroPadding2D, Dropout, Activation, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)].T\n",
    "    return Y\n",
    "\n",
    "file = h5py.File('data/DB1_S1_image.h5','r')\n",
    "imageData   = file['imageData'][:]\n",
    "imageLabel  = file['imageLabel'][:]  \n",
    "file.close()\n",
    "\n",
    "# 随机打乱数据和标签\n",
    "N = imageData.shape[0]\n",
    "index = np.random.permutation(N)\n",
    "data  = imageData[index,:,:]\n",
    "label = imageLabel[index]\n",
    "\n",
    "# 对数据升维,标签one-hot\n",
    "data  = np.expand_dims(data, axis=3)\n",
    "label = convert_to_one_hot(label,52).T\n",
    "\n",
    "# 划分数据集\n",
    "N = data.shape[0]\n",
    "num_train = round(N*0.8)\n",
    "X_train = data[0:num_train,:,:,:]\n",
    "Y_train = label[0:num_train,:]\n",
    "X_test  = data[num_train:N,:,:,:]\n",
    "Y_test  = label[num_train:N,:]\n",
    "\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#写一个LossHistory类，保存loss和acc\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = {'batch':[], 'epoch':[]}\n",
    "        self.accuracy = {'batch':[], 'epoch':[]}\n",
    "        self.val_loss = {'batch':[], 'epoch':[]}\n",
    "        self.val_acc = {'batch':[], 'epoch':[]}\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses['batch'].append(logs.get('loss'))\n",
    "        self.accuracy['batch'].append(logs.get('acc'))\n",
    "        self.val_loss['batch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['batch'].append(logs.get('val_acc'))\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses['epoch'].append(logs.get('loss'))\n",
    "        self.accuracy['epoch'].append(logs.get('acc'))\n",
    "        self.val_loss['epoch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['epoch'].append(logs.get('val_acc'))\n",
    "\n",
    "    def loss_plot(self, loss_type):\n",
    "        iters = range(len(self.losses[loss_type]))\n",
    "        plt.figure()\n",
    "        # acc\n",
    "        plt.plot(iters, self.accuracy[loss_type], 'r', label='train acc')\n",
    "        # loss\n",
    "        plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n",
    "        if loss_type == 'epoch':\n",
    "            # val_acc\n",
    "            plt.plot(iters, self.val_acc[loss_type], 'b', label='val acc')\n",
    "            # val_loss\n",
    "            plt.plot(iters, self.val_loss[loss_type], 'k', label='val loss')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(loss_type)\n",
    "        plt.ylabel('acc-loss')\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 12, 10, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 12, 10, 32)        320       \n",
      "_________________________________________________________________\n",
      "relu1 (Activation)           (None, 12, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 12, 10, 64)        18496     \n",
      "_________________________________________________________________\n",
      "relu2 (Activation)           (None, 12, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling2D)         (None, 6, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 6, 5, 128)         73856     \n",
      "_________________________________________________________________\n",
      "relu3 (Activation)           (None, 6, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 6, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "pool2 (MaxPooling2D)         (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 128)               147584    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 52)                6708      \n",
      "=================================================================\n",
      "Total params: 246,964\n",
      "Trainable params: 246,964\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def CNN(input_shape=(12,10,1), classes=52): \n",
    "    X_input = Input(input_shape)\n",
    "    \n",
    "    X = Conv2D(filters=32, kernel_size=(3,3), strides=(1,1),padding='same', name='conv1')(X_input)\n",
    "    X = Activation('relu', name='relu1')(X)\n",
    "\n",
    "    X = Conv2D(filters=64, kernel_size=(3,3), strides=(1,1),padding='same', name='conv2')(X)\n",
    "    X = Activation('relu', name='relu2')(X)\n",
    "    X = MaxPooling2D((2,2), strides=(2,2), name='pool1')(X)\n",
    "    \n",
    "    X = Conv2D(filters=128, kernel_size=(3,3), strides=(1,1),padding='same', name='conv3')(X)\n",
    "    X = Activation('relu', name='relu3')(X)\n",
    "    \n",
    "    X = ZeroPadding2D((0,1))(X)\n",
    "    X = MaxPooling2D((2,2), strides=(2,2), name='pool2')(X)\n",
    "    \n",
    "    X = Flatten(name='flatten')(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = Dense(128,activation='relu',name='fc1')(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = Dense(classes, activation='softmax', name='fc2')(X)\n",
    "    \n",
    "    model = Model(inputs=X_input, outputs=X, name='CNN')\n",
    "    return model\n",
    "    \n",
    "model = CNN(input_shape = (12, 10, 1), classes = 52)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练原始数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12038 samples, validate on 3009 samples\n",
      "Epoch 1/200\n",
      "12038/12038 [==============================] - 3s 281us/step - loss: 3.5113 - acc: 0.1009 - val_loss: 2.7274 - val_acc: 0.3151\n",
      "Epoch 2/200\n",
      "12038/12038 [==============================] - 3s 230us/step - loss: 2.6762 - acc: 0.2792 - val_loss: 2.1238 - val_acc: 0.4380\n",
      "Epoch 3/200\n",
      "12038/12038 [==============================] - 3s 214us/step - loss: 2.2552 - acc: 0.3833 - val_loss: 1.8534 - val_acc: 0.5012\n",
      "Epoch 4/200\n",
      "12038/12038 [==============================] - 3s 214us/step - loss: 2.0147 - acc: 0.4388 - val_loss: 1.5852 - val_acc: 0.5670\n",
      "Epoch 5/200\n",
      "12038/12038 [==============================] - 3s 215us/step - loss: 1.8340 - acc: 0.4805 - val_loss: 1.3982 - val_acc: 0.5995\n",
      "Epoch 6/200\n",
      "12038/12038 [==============================] - 3s 218us/step - loss: 1.7051 - acc: 0.5100 - val_loss: 1.3327 - val_acc: 0.6268\n",
      "Epoch 7/200\n",
      "12038/12038 [==============================] - 3s 215us/step - loss: 1.5952 - acc: 0.5404 - val_loss: 1.2738 - val_acc: 0.6275\n",
      "Epoch 8/200\n",
      "12038/12038 [==============================] - 3s 215us/step - loss: 1.5325 - acc: 0.5486 - val_loss: 1.2015 - val_acc: 0.6484\n",
      "Epoch 9/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 1.4798 - acc: 0.5634 - val_loss: 1.1773 - val_acc: 0.6504\n",
      "Epoch 10/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 1.4431 - acc: 0.5751 - val_loss: 1.1491 - val_acc: 0.6590\n",
      "Epoch 11/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 1.3949 - acc: 0.5826 - val_loss: 1.1073 - val_acc: 0.6756\n",
      "Epoch 12/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 1.3566 - acc: 0.5930 - val_loss: 1.0775 - val_acc: 0.6743\n",
      "Epoch 13/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 1.3267 - acc: 0.6047 - val_loss: 1.0575 - val_acc: 0.6793\n",
      "Epoch 14/200\n",
      "12038/12038 [==============================] - 3s 215us/step - loss: 1.2844 - acc: 0.6116 - val_loss: 1.0392 - val_acc: 0.6853\n",
      "Epoch 15/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 1.2700 - acc: 0.6175 - val_loss: 1.0081 - val_acc: 0.6999\n",
      "Epoch 16/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 1.2438 - acc: 0.6232 - val_loss: 1.0146 - val_acc: 0.6913\n",
      "Epoch 17/200\n",
      "12038/12038 [==============================] - 3s 218us/step - loss: 1.2312 - acc: 0.6236 - val_loss: 0.9769 - val_acc: 0.7009\n",
      "Epoch 18/200\n",
      "12038/12038 [==============================] - 3s 213us/step - loss: 1.2007 - acc: 0.6323 - val_loss: 0.9669 - val_acc: 0.7046\n",
      "Epoch 19/200\n",
      "12038/12038 [==============================] - 3s 215us/step - loss: 1.1853 - acc: 0.6372 - val_loss: 0.9722 - val_acc: 0.7009\n",
      "Epoch 20/200\n",
      "12038/12038 [==============================] - 3s 219us/step - loss: 1.1663 - acc: 0.6477 - val_loss: 0.9302 - val_acc: 0.7159\n",
      "Epoch 21/200\n",
      "12038/12038 [==============================] - 3s 215us/step - loss: 1.1266 - acc: 0.6533 - val_loss: 0.9187 - val_acc: 0.7198\n",
      "Epoch 22/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 1.1357 - acc: 0.6527 - val_loss: 0.9291 - val_acc: 0.7162\n",
      "Epoch 23/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 1.1245 - acc: 0.6551 - val_loss: 0.9273 - val_acc: 0.7085\n",
      "Epoch 24/200\n",
      "12038/12038 [==============================] - 3s 215us/step - loss: 1.0859 - acc: 0.6606 - val_loss: 0.9112 - val_acc: 0.7182\n",
      "Epoch 25/200\n",
      "12038/12038 [==============================] - 3s 218us/step - loss: 1.0889 - acc: 0.6602 - val_loss: 0.8904 - val_acc: 0.7291\n",
      "Epoch 26/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 1.0798 - acc: 0.6667 - val_loss: 0.8866 - val_acc: 0.7295\n",
      "Epoch 27/200\n",
      "12038/12038 [==============================] - 3s 214us/step - loss: 1.0620 - acc: 0.6684 - val_loss: 0.8971 - val_acc: 0.7265\n",
      "Epoch 28/200\n",
      "12038/12038 [==============================] - 3s 215us/step - loss: 1.0524 - acc: 0.6722 - val_loss: 0.8868 - val_acc: 0.7305\n",
      "Epoch 29/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 1.0258 - acc: 0.6793 - val_loss: 0.8719 - val_acc: 0.7305\n",
      "Epoch 30/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 1.0382 - acc: 0.6766 - val_loss: 0.8841 - val_acc: 0.7248\n",
      "Epoch 31/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 1.0082 - acc: 0.6838 - val_loss: 0.8701 - val_acc: 0.7318\n",
      "Epoch 32/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 1.0067 - acc: 0.6832 - val_loss: 0.8631 - val_acc: 0.7311\n",
      "Epoch 33/200\n",
      "12038/12038 [==============================] - 3s 215us/step - loss: 1.0056 - acc: 0.6813 - val_loss: 0.8572 - val_acc: 0.7351\n",
      "Epoch 34/200\n",
      "12038/12038 [==============================] - 3s 215us/step - loss: 0.9898 - acc: 0.6867 - val_loss: 0.8638 - val_acc: 0.7341\n",
      "Epoch 35/200\n",
      "12038/12038 [==============================] - 3s 214us/step - loss: 0.9833 - acc: 0.6906 - val_loss: 0.8472 - val_acc: 0.7321\n",
      "Epoch 36/200\n",
      "12038/12038 [==============================] - 3s 214us/step - loss: 0.9703 - acc: 0.6945 - val_loss: 0.8456 - val_acc: 0.7358\n",
      "Epoch 37/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 0.9588 - acc: 0.6955 - val_loss: 0.8266 - val_acc: 0.7385\n",
      "Epoch 38/200\n",
      "12038/12038 [==============================] - 3s 215us/step - loss: 0.9599 - acc: 0.6965 - val_loss: 0.8334 - val_acc: 0.7335\n",
      "Epoch 39/200\n",
      "12038/12038 [==============================] - 3s 215us/step - loss: 0.9435 - acc: 0.7039 - val_loss: 0.8490 - val_acc: 0.7341\n",
      "Epoch 40/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.9345 - acc: 0.7015 - val_loss: 0.8218 - val_acc: 0.7414\n",
      "Epoch 41/200\n",
      "12038/12038 [==============================] - 3s 214us/step - loss: 0.9254 - acc: 0.7042 - val_loss: 0.8313 - val_acc: 0.7368\n",
      "Epoch 42/200\n",
      "12038/12038 [==============================] - 3s 215us/step - loss: 0.9431 - acc: 0.7031 - val_loss: 0.8379 - val_acc: 0.7368\n",
      "Epoch 43/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.9115 - acc: 0.7055 - val_loss: 0.8235 - val_acc: 0.7471\n",
      "Epoch 44/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 0.9115 - acc: 0.7124 - val_loss: 0.8072 - val_acc: 0.7418\n",
      "Epoch 45/200\n",
      "12038/12038 [==============================] - 3s 215us/step - loss: 0.9118 - acc: 0.7078 - val_loss: 0.8180 - val_acc: 0.7468\n",
      "Epoch 46/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 0.9069 - acc: 0.7155 - val_loss: 0.7990 - val_acc: 0.7488\n",
      "Epoch 47/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.8910 - acc: 0.7120 - val_loss: 0.8190 - val_acc: 0.7418\n",
      "Epoch 48/200\n",
      "12038/12038 [==============================] - 3s 215us/step - loss: 0.8844 - acc: 0.7206 - val_loss: 0.8075 - val_acc: 0.7461\n",
      "Epoch 49/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.8747 - acc: 0.7171 - val_loss: 0.8174 - val_acc: 0.7388\n",
      "Epoch 50/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.8663 - acc: 0.7234 - val_loss: 0.7991 - val_acc: 0.7554\n",
      "Epoch 51/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 0.8567 - acc: 0.7276 - val_loss: 0.8014 - val_acc: 0.7501\n",
      "Epoch 52/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.8658 - acc: 0.7235 - val_loss: 0.7937 - val_acc: 0.7488\n",
      "Epoch 53/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.8576 - acc: 0.7234 - val_loss: 0.8014 - val_acc: 0.7504\n",
      "Epoch 54/200\n",
      "12038/12038 [==============================] - 3s 215us/step - loss: 0.8467 - acc: 0.7290 - val_loss: 0.7924 - val_acc: 0.7488\n",
      "Epoch 55/200\n",
      "12038/12038 [==============================] - 3s 219us/step - loss: 0.8380 - acc: 0.7299 - val_loss: 0.7762 - val_acc: 0.7551\n",
      "Epoch 56/200\n",
      "12038/12038 [==============================] - 3s 214us/step - loss: 0.8465 - acc: 0.7274 - val_loss: 0.8023 - val_acc: 0.7498\n",
      "Epoch 57/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.8369 - acc: 0.7242 - val_loss: 0.7789 - val_acc: 0.7577\n",
      "Epoch 58/200\n",
      "12038/12038 [==============================] - 3s 214us/step - loss: 0.8259 - acc: 0.7364 - val_loss: 0.7923 - val_acc: 0.7547\n",
      "Epoch 59/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.8140 - acc: 0.7382 - val_loss: 0.7919 - val_acc: 0.7524\n",
      "Epoch 60/200\n",
      "12038/12038 [==============================] - 3s 215us/step - loss: 0.8328 - acc: 0.7368 - val_loss: 0.7576 - val_acc: 0.7551\n",
      "Epoch 61/200\n",
      "12038/12038 [==============================] - 3s 213us/step - loss: 0.8049 - acc: 0.7361 - val_loss: 0.7707 - val_acc: 0.7574\n",
      "Epoch 62/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.8030 - acc: 0.7414 - val_loss: 0.7955 - val_acc: 0.7531\n",
      "Epoch 63/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 0.8037 - acc: 0.7386 - val_loss: 0.7786 - val_acc: 0.7574\n",
      "Epoch 64/200\n",
      "12038/12038 [==============================] - 3s 218us/step - loss: 0.8072 - acc: 0.7382 - val_loss: 0.7804 - val_acc: 0.7584\n",
      "Epoch 65/200\n",
      "12038/12038 [==============================] - 3s 215us/step - loss: 0.7899 - acc: 0.7446 - val_loss: 0.7857 - val_acc: 0.7504\n",
      "Epoch 66/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 0.7803 - acc: 0.7493 - val_loss: 0.7708 - val_acc: 0.7601\n",
      "Epoch 67/200\n",
      "12038/12038 [==============================] - 3s 218us/step - loss: 0.7925 - acc: 0.7437 - val_loss: 0.7701 - val_acc: 0.7647\n",
      "Epoch 68/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 0.7782 - acc: 0.7470 - val_loss: 0.7659 - val_acc: 0.7611\n",
      "Epoch 69/200\n",
      "12038/12038 [==============================] - 3s 218us/step - loss: 0.7628 - acc: 0.7527 - val_loss: 0.7672 - val_acc: 0.7627\n",
      "Epoch 70/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.7599 - acc: 0.7520 - val_loss: 0.7513 - val_acc: 0.7644\n",
      "Epoch 71/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 0.7811 - acc: 0.7478 - val_loss: 0.7623 - val_acc: 0.7637\n",
      "Epoch 72/200\n",
      "12038/12038 [==============================] - 3s 218us/step - loss: 0.7665 - acc: 0.7475 - val_loss: 0.7518 - val_acc: 0.7581\n",
      "Epoch 73/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.7655 - acc: 0.7490 - val_loss: 0.7688 - val_acc: 0.7594\n",
      "Epoch 74/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.7537 - acc: 0.7548 - val_loss: 0.7600 - val_acc: 0.7634\n",
      "Epoch 75/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 0.7544 - acc: 0.7531 - val_loss: 0.7582 - val_acc: 0.7614\n",
      "Epoch 76/200\n",
      "12038/12038 [==============================] - 3s 214us/step - loss: 0.7445 - acc: 0.7578 - val_loss: 0.7500 - val_acc: 0.7677\n",
      "Epoch 77/200\n",
      "12038/12038 [==============================] - 3s 220us/step - loss: 0.7425 - acc: 0.7576 - val_loss: 0.7532 - val_acc: 0.7624\n",
      "Epoch 78/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.7604 - acc: 0.7517 - val_loss: 0.7611 - val_acc: 0.7700\n",
      "Epoch 79/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.7423 - acc: 0.7577 - val_loss: 0.7691 - val_acc: 0.7564\n",
      "Epoch 80/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 0.7577 - acc: 0.7548 - val_loss: 0.7521 - val_acc: 0.7664\n",
      "Epoch 81/200\n",
      "12038/12038 [==============================] - 3s 215us/step - loss: 0.7195 - acc: 0.7612 - val_loss: 0.7458 - val_acc: 0.7743\n",
      "Epoch 82/200\n",
      "12038/12038 [==============================] - 3s 218us/step - loss: 0.7416 - acc: 0.7540 - val_loss: 0.7547 - val_acc: 0.7714\n",
      "Epoch 83/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.7253 - acc: 0.7579 - val_loss: 0.7605 - val_acc: 0.7684\n",
      "Epoch 84/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 0.7121 - acc: 0.7628 - val_loss: 0.7502 - val_acc: 0.7677\n",
      "Epoch 85/200\n",
      "12038/12038 [==============================] - 3s 215us/step - loss: 0.7291 - acc: 0.7632 - val_loss: 0.7451 - val_acc: 0.7730\n",
      "Epoch 86/200\n",
      "12038/12038 [==============================] - 3s 219us/step - loss: 0.7160 - acc: 0.7612 - val_loss: 0.7398 - val_acc: 0.7650\n",
      "Epoch 87/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 0.7150 - acc: 0.7704 - val_loss: 0.7328 - val_acc: 0.7647\n",
      "Epoch 88/200\n",
      "12038/12038 [==============================] - 3s 215us/step - loss: 0.7028 - acc: 0.7687 - val_loss: 0.7501 - val_acc: 0.7654\n",
      "Epoch 89/200\n",
      "12038/12038 [==============================] - 3s 218us/step - loss: 0.7095 - acc: 0.7670 - val_loss: 0.7523 - val_acc: 0.7760\n",
      "Epoch 90/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 0.6943 - acc: 0.7672 - val_loss: 0.7650 - val_acc: 0.7650\n",
      "Epoch 91/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.7045 - acc: 0.7684 - val_loss: 0.7534 - val_acc: 0.7690\n",
      "Epoch 92/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 0.6882 - acc: 0.7720 - val_loss: 0.7731 - val_acc: 0.7677\n",
      "Epoch 93/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 0.6845 - acc: 0.7728 - val_loss: 0.7701 - val_acc: 0.7674\n",
      "Epoch 94/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 0.6951 - acc: 0.7710 - val_loss: 0.7557 - val_acc: 0.7664\n",
      "Epoch 95/200\n",
      "12038/12038 [==============================] - 3s 218us/step - loss: 0.6913 - acc: 0.7730 - val_loss: 0.7375 - val_acc: 0.7707\n",
      "Epoch 96/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 0.6682 - acc: 0.7760 - val_loss: 0.7546 - val_acc: 0.7697\n",
      "Epoch 97/200\n",
      "12038/12038 [==============================] - 3s 219us/step - loss: 0.6874 - acc: 0.7692 - val_loss: 0.7438 - val_acc: 0.7797\n",
      "Epoch 98/200\n",
      "12038/12038 [==============================] - 3s 219us/step - loss: 0.6914 - acc: 0.7680 - val_loss: 0.7468 - val_acc: 0.7790\n",
      "Epoch 99/200\n",
      "12038/12038 [==============================] - 3s 218us/step - loss: 0.6820 - acc: 0.7738 - val_loss: 0.7764 - val_acc: 0.7763\n",
      "Epoch 100/200\n",
      "12038/12038 [==============================] - 3s 218us/step - loss: 0.6814 - acc: 0.7715 - val_loss: 0.7287 - val_acc: 0.7817\n",
      "Epoch 101/200\n",
      "12038/12038 [==============================] - 3s 215us/step - loss: 0.6661 - acc: 0.7778 - val_loss: 0.7443 - val_acc: 0.7737\n",
      "Epoch 102/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 0.6667 - acc: 0.7773 - val_loss: 0.7320 - val_acc: 0.7767\n",
      "Epoch 103/200\n",
      "12038/12038 [==============================] - 3s 219us/step - loss: 0.6538 - acc: 0.7856 - val_loss: 0.7355 - val_acc: 0.7777\n",
      "Epoch 104/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 0.6574 - acc: 0.7814 - val_loss: 0.7436 - val_acc: 0.7697\n",
      "Epoch 105/200\n",
      "12038/12038 [==============================] - 3s 215us/step - loss: 0.6437 - acc: 0.7871 - val_loss: 0.7524 - val_acc: 0.7783\n",
      "Epoch 106/200\n",
      "12038/12038 [==============================] - 3s 215us/step - loss: 0.6568 - acc: 0.7834 - val_loss: 0.7490 - val_acc: 0.7760\n",
      "Epoch 107/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 0.6634 - acc: 0.7801 - val_loss: 0.7350 - val_acc: 0.7823\n",
      "Epoch 108/200\n",
      "12038/12038 [==============================] - 3s 218us/step - loss: 0.6808 - acc: 0.7770 - val_loss: 0.7468 - val_acc: 0.7810\n",
      "Epoch 109/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.6659 - acc: 0.7826 - val_loss: 0.7363 - val_acc: 0.7783\n",
      "Epoch 110/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 0.6414 - acc: 0.7848 - val_loss: 0.7551 - val_acc: 0.7780\n",
      "Epoch 111/200\n",
      "12038/12038 [==============================] - 3s 214us/step - loss: 0.6542 - acc: 0.7825 - val_loss: 0.7381 - val_acc: 0.7773\n",
      "Epoch 112/200\n",
      "12038/12038 [==============================] - 3s 218us/step - loss: 0.6545 - acc: 0.7829 - val_loss: 0.7314 - val_acc: 0.7787\n",
      "Epoch 113/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 0.6423 - acc: 0.7888 - val_loss: 0.7459 - val_acc: 0.7727\n",
      "Epoch 114/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.6334 - acc: 0.7884 - val_loss: 0.7475 - val_acc: 0.7767\n",
      "Epoch 115/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.6386 - acc: 0.7911 - val_loss: 0.7429 - val_acc: 0.7767\n",
      "Epoch 116/200\n",
      "12038/12038 [==============================] - 3s 213us/step - loss: 0.6249 - acc: 0.7918 - val_loss: 0.7885 - val_acc: 0.7650\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12038/12038 [==============================] - 3s 215us/step - loss: 0.6322 - acc: 0.7888 - val_loss: 0.7473 - val_acc: 0.7710\n",
      "Epoch 118/200\n",
      "12038/12038 [==============================] - 3s 214us/step - loss: 0.6227 - acc: 0.7940 - val_loss: 0.7546 - val_acc: 0.7690\n",
      "Epoch 119/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 0.6384 - acc: 0.7866 - val_loss: 0.7512 - val_acc: 0.7797\n",
      "Epoch 120/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 0.6191 - acc: 0.7918 - val_loss: 0.7616 - val_acc: 0.7773\n",
      "Epoch 121/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.6247 - acc: 0.7917 - val_loss: 0.7608 - val_acc: 0.7737\n",
      "Epoch 122/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 0.6252 - acc: 0.7928 - val_loss: 0.7517 - val_acc: 0.7803\n",
      "Epoch 123/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.6193 - acc: 0.7944 - val_loss: 0.7342 - val_acc: 0.7813\n",
      "Epoch 124/200\n",
      "12038/12038 [==============================] - 3s 218us/step - loss: 0.6190 - acc: 0.7984 - val_loss: 0.7394 - val_acc: 0.7790\n",
      "Epoch 125/200\n",
      "12038/12038 [==============================] - 3s 218us/step - loss: 0.6166 - acc: 0.7958 - val_loss: 0.7579 - val_acc: 0.7737\n",
      "Epoch 126/200\n",
      "12038/12038 [==============================] - 3s 219us/step - loss: 0.6126 - acc: 0.7944 - val_loss: 0.7637 - val_acc: 0.7793\n",
      "Epoch 127/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 0.6079 - acc: 0.7954 - val_loss: 0.7739 - val_acc: 0.7714\n",
      "Epoch 128/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 0.6161 - acc: 0.7945 - val_loss: 0.7740 - val_acc: 0.7690\n",
      "Epoch 129/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.6036 - acc: 0.7969 - val_loss: 0.7729 - val_acc: 0.7740\n",
      "Epoch 130/200\n",
      "12038/12038 [==============================] - 3s 215us/step - loss: 0.6112 - acc: 0.7950 - val_loss: 0.7488 - val_acc: 0.7733\n",
      "Epoch 131/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 0.6107 - acc: 0.7979 - val_loss: 0.7446 - val_acc: 0.7807\n",
      "Epoch 132/200\n",
      "12038/12038 [==============================] - 3s 219us/step - loss: 0.6107 - acc: 0.7950 - val_loss: 0.7644 - val_acc: 0.7753\n",
      "Epoch 133/200\n",
      "12038/12038 [==============================] - 3s 215us/step - loss: 0.5986 - acc: 0.8045 - val_loss: 0.7452 - val_acc: 0.7846\n",
      "Epoch 134/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.5987 - acc: 0.8006 - val_loss: 0.7525 - val_acc: 0.7743\n",
      "Epoch 135/200\n",
      "12038/12038 [==============================] - 3s 215us/step - loss: 0.5904 - acc: 0.8041 - val_loss: 0.7555 - val_acc: 0.7727\n",
      "Epoch 136/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.5925 - acc: 0.8030 - val_loss: 0.7524 - val_acc: 0.7793\n",
      "Epoch 137/200\n",
      "12038/12038 [==============================] - 3s 219us/step - loss: 0.6020 - acc: 0.8025 - val_loss: 0.7687 - val_acc: 0.7743\n",
      "Epoch 138/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.5909 - acc: 0.8053 - val_loss: 0.7714 - val_acc: 0.7733\n",
      "Epoch 139/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 0.5924 - acc: 0.8009 - val_loss: 0.7505 - val_acc: 0.7797\n",
      "Epoch 140/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.5890 - acc: 0.8006 - val_loss: 0.7466 - val_acc: 0.7780\n",
      "Epoch 141/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 0.5805 - acc: 0.8053 - val_loss: 0.7371 - val_acc: 0.7876\n",
      "Epoch 142/200\n",
      "12038/12038 [==============================] - 3s 219us/step - loss: 0.5958 - acc: 0.8036 - val_loss: 0.7724 - val_acc: 0.7896\n",
      "Epoch 143/200\n",
      "12038/12038 [==============================] - 3s 218us/step - loss: 0.5837 - acc: 0.8052 - val_loss: 0.7331 - val_acc: 0.7836\n",
      "Epoch 144/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 0.5785 - acc: 0.7976 - val_loss: 0.7428 - val_acc: 0.7823\n",
      "Epoch 145/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.5739 - acc: 0.8070 - val_loss: 0.7379 - val_acc: 0.7793\n",
      "Epoch 146/200\n",
      "12038/12038 [==============================] - 3s 218us/step - loss: 0.5687 - acc: 0.8053 - val_loss: 0.7521 - val_acc: 0.7863\n",
      "Epoch 147/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.5788 - acc: 0.8018 - val_loss: 0.7633 - val_acc: 0.7707\n",
      "Epoch 148/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.5613 - acc: 0.8100 - val_loss: 0.7625 - val_acc: 0.7823\n",
      "Epoch 149/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.5871 - acc: 0.8045 - val_loss: 0.7579 - val_acc: 0.7780\n",
      "Epoch 150/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 0.5574 - acc: 0.8113 - val_loss: 0.7522 - val_acc: 0.7833\n",
      "Epoch 151/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.5778 - acc: 0.8098 - val_loss: 0.7582 - val_acc: 0.7827\n",
      "Epoch 152/200\n",
      "12038/12038 [==============================] - 3s 219us/step - loss: 0.5848 - acc: 0.8040 - val_loss: 0.7489 - val_acc: 0.7856\n",
      "Epoch 153/200\n",
      "12038/12038 [==============================] - 3s 218us/step - loss: 0.5774 - acc: 0.8054 - val_loss: 0.7587 - val_acc: 0.7793\n",
      "Epoch 154/200\n",
      "12038/12038 [==============================] - 3s 214us/step - loss: 0.5531 - acc: 0.8148 - val_loss: 0.7561 - val_acc: 0.7890\n",
      "Epoch 155/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.5459 - acc: 0.8187 - val_loss: 0.7719 - val_acc: 0.7777\n",
      "Epoch 156/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 0.5586 - acc: 0.8120 - val_loss: 0.7732 - val_acc: 0.7823\n",
      "Epoch 157/200\n",
      "12038/12038 [==============================] - 3s 213us/step - loss: 0.5401 - acc: 0.8192 - val_loss: 0.7655 - val_acc: 0.7817\n",
      "Epoch 158/200\n",
      "12038/12038 [==============================] - 3s 218us/step - loss: 0.5563 - acc: 0.8180 - val_loss: 0.7526 - val_acc: 0.7817\n",
      "Epoch 159/200\n",
      "12038/12038 [==============================] - 3s 213us/step - loss: 0.5504 - acc: 0.8144 - val_loss: 0.7662 - val_acc: 0.7820\n",
      "Epoch 160/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 0.5541 - acc: 0.8121 - val_loss: 0.7667 - val_acc: 0.7797\n",
      "Epoch 161/200\n",
      "12038/12038 [==============================] - 3s 220us/step - loss: 0.5511 - acc: 0.8135 - val_loss: 0.7448 - val_acc: 0.7840\n",
      "Epoch 162/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.5575 - acc: 0.8138 - val_loss: 0.7546 - val_acc: 0.7827\n",
      "Epoch 163/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.5438 - acc: 0.8183 - val_loss: 0.7585 - val_acc: 0.7843\n",
      "Epoch 164/200\n",
      "12038/12038 [==============================] - 3s 220us/step - loss: 0.5404 - acc: 0.8158 - val_loss: 0.7509 - val_acc: 0.7820\n",
      "Epoch 165/200\n",
      "12038/12038 [==============================] - 3s 215us/step - loss: 0.5527 - acc: 0.8121 - val_loss: 0.7417 - val_acc: 0.7886\n",
      "Epoch 166/200\n",
      "12038/12038 [==============================] - 3s 215us/step - loss: 0.5574 - acc: 0.8107 - val_loss: 0.7588 - val_acc: 0.7896\n",
      "Epoch 167/200\n",
      "12038/12038 [==============================] - 3s 214us/step - loss: 0.5424 - acc: 0.8167 - val_loss: 0.7510 - val_acc: 0.7850\n",
      "Epoch 168/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.5355 - acc: 0.8202 - val_loss: 0.7515 - val_acc: 0.7833\n",
      "Epoch 169/200\n",
      "12038/12038 [==============================] - 3s 218us/step - loss: 0.5470 - acc: 0.8162 - val_loss: 0.7659 - val_acc: 0.7817\n",
      "Epoch 170/200\n",
      "12038/12038 [==============================] - 3s 215us/step - loss: 0.5419 - acc: 0.8173 - val_loss: 0.7690 - val_acc: 0.7810\n",
      "Epoch 171/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 0.5313 - acc: 0.8202 - val_loss: 0.7638 - val_acc: 0.7827\n",
      "Epoch 172/200\n",
      "12038/12038 [==============================] - 3s 215us/step - loss: 0.5488 - acc: 0.8175 - val_loss: 0.7552 - val_acc: 0.7893\n",
      "Epoch 173/200\n",
      "12038/12038 [==============================] - 3s 218us/step - loss: 0.5401 - acc: 0.8194 - val_loss: 0.7551 - val_acc: 0.7883\n",
      "Epoch 174/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 0.5302 - acc: 0.8224 - val_loss: 0.7661 - val_acc: 0.7803\n",
      "Epoch 175/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12038/12038 [==============================] - 3s 217us/step - loss: 0.5354 - acc: 0.8202 - val_loss: 0.7678 - val_acc: 0.7853\n",
      "Epoch 176/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.5275 - acc: 0.8187 - val_loss: 0.7868 - val_acc: 0.7810\n",
      "Epoch 177/200\n",
      "12038/12038 [==============================] - 3s 215us/step - loss: 0.5223 - acc: 0.8249 - val_loss: 0.7853 - val_acc: 0.7846\n",
      "Epoch 178/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 0.5586 - acc: 0.8155 - val_loss: 0.7496 - val_acc: 0.7873\n",
      "Epoch 179/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 0.5415 - acc: 0.8223 - val_loss: 0.7583 - val_acc: 0.7840\n",
      "Epoch 180/200\n",
      "12038/12038 [==============================] - 3s 215us/step - loss: 0.5333 - acc: 0.8215 - val_loss: 0.7781 - val_acc: 0.7856\n",
      "Epoch 181/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.5244 - acc: 0.8215 - val_loss: 0.7733 - val_acc: 0.7846\n",
      "Epoch 182/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.5400 - acc: 0.8191 - val_loss: 0.7866 - val_acc: 0.7813\n",
      "Epoch 183/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.5169 - acc: 0.8251 - val_loss: 0.7779 - val_acc: 0.7840\n",
      "Epoch 184/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.5268 - acc: 0.8192 - val_loss: 0.7567 - val_acc: 0.7813\n",
      "Epoch 185/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.5315 - acc: 0.8229 - val_loss: 0.7550 - val_acc: 0.7876\n",
      "Epoch 186/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 0.5183 - acc: 0.8249 - val_loss: 0.7667 - val_acc: 0.7846\n",
      "Epoch 187/200\n",
      "12038/12038 [==============================] - 3s 219us/step - loss: 0.5224 - acc: 0.8249 - val_loss: 0.7578 - val_acc: 0.7860\n",
      "Epoch 188/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.5074 - acc: 0.8310 - val_loss: 0.7628 - val_acc: 0.7823\n",
      "Epoch 189/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 0.5244 - acc: 0.8177 - val_loss: 0.7714 - val_acc: 0.7827\n",
      "Epoch 190/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.5038 - acc: 0.8323 - val_loss: 0.7593 - val_acc: 0.7863\n",
      "Epoch 191/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.5327 - acc: 0.8193 - val_loss: 0.7436 - val_acc: 0.7926\n",
      "Epoch 192/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 0.5258 - acc: 0.8222 - val_loss: 0.7335 - val_acc: 0.7940\n",
      "Epoch 193/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 0.5125 - acc: 0.8296 - val_loss: 0.7805 - val_acc: 0.7880\n",
      "Epoch 194/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 0.5212 - acc: 0.8226 - val_loss: 0.7713 - val_acc: 0.7833\n",
      "Epoch 195/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.5072 - acc: 0.8301 - val_loss: 0.7764 - val_acc: 0.7810\n",
      "Epoch 196/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.5088 - acc: 0.8302 - val_loss: 0.7694 - val_acc: 0.7903\n",
      "Epoch 197/200\n",
      "12038/12038 [==============================] - 3s 215us/step - loss: 0.5042 - acc: 0.8267 - val_loss: 0.7889 - val_acc: 0.7860\n",
      "Epoch 198/200\n",
      "12038/12038 [==============================] - 3s 217us/step - loss: 0.5049 - acc: 0.8285 - val_loss: 0.7872 - val_acc: 0.7920\n",
      "Epoch 199/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.5006 - acc: 0.8294 - val_loss: 0.7623 - val_acc: 0.7903\n",
      "Epoch 200/200\n",
      "12038/12038 [==============================] - 3s 216us/step - loss: 0.5026 - acc: 0.8270 - val_loss: 0.7557 - val_acc: 0.7873\n",
      "12038/12038 [==============================] - 2s 145us/step\n",
      "Train Loss = 0.16006244075721157\n",
      "Train Accuracy = 0.956221963781359\n",
      "3009/3009 [==============================] - 0s 150us/step\n",
      "Test Loss = 0.7556592999663775\n",
      "Test Accuracy = 0.7873047524094383\n",
      "time: 525.0024869441986\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = LossHistory() # 创建一个history实例\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=200, batch_size=64, verbose=1, \n",
    "            validation_data=(X_test, Y_test),callbacks=[history])\n",
    "\n",
    "preds_train = model.evaluate(X_train, Y_train)\n",
    "print(\"Train Loss = \" + str(preds_train[0]))\n",
    "print(\"Train Accuracy = \" + str(preds_train[1]))\n",
    "\n",
    "preds_test  = model.evaluate(X_test, Y_test)\n",
    "print(\"Test Loss = \" + str(preds_test[0]))\n",
    "print(\"Test Accuracy = \" + str(preds_test[1]))\n",
    "\n",
    "end = time.time()\n",
    "print(\"time:\",end-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6a50495908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history.loss_plot('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
